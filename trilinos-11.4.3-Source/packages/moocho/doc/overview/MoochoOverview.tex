%
% $Id$
% A template to build SAND reports. See the examples for more details and
% formatting suggestions. A command reference is available at
% http://www.cs.sandia.gov/~rolf/SANDreport
%
\documentclass[pdf,12pt,report]{SANDreport}
%Local stuff
\usepackage{graphicx}
\usepackage{latexsym}
\input{rab_commands}

% ---------------------------------------------------------------------------- %
% Set the title, author, and date
%

\title{
\center
Mathematical and High-Level Overview of MOOCHO \\[2ex]
The Multifunctional Object-Oriented arCHitecture for Optimization
}
\author{
Roscoe A. Bartlett \\ Department of Optimization and Uncertainty Estimation \\ \\
Sandia National Laboratories\footnote{ Sandia is a multiprogram laboratory
operated by Sandia Corporation, a Lockheed-Martin Company, for the United States
Department of Energy under Contract DE-AC04-94AL85000.},
Albuquerque NM 87185 USA, \\ }
\date{}

% ---------------------------------------------------------------------------- %
% These are mandatory
%
\SANDnum{SAND2009-3969}	% e.g. \SANDnum{SAND2009-xxx}
\SANDprintDate{Jine 2009}
\SANDauthor{Roscoe A. Bartlett} % One line, separated by commas

% ---------------------------------------------------------------------------- %
% These are optional
%
%\SANDrePrintDate{}	% May be repeated for successive printings
%\SANDsupersed{}{}	% {Old SAND number}{Old date}


% ---------------------------------------------------------------------------- %
% Build your markings. See example files and SAND Report Guide
%
    %\SANDreleaseType{}
    %\SANDmarkTopBottomCoverBackTitle{}
    %\SANDmarkBottomCover{}
    %\SANDmarkTopBottomCoverTitle{}
    %\SANDmarkTop{}
    %\SANDmarkBottom{}
    %\SANDmarkTopBottom{}
    %\SANDmarkCover{}
    %\SANDmarkCoverTitle{}


% ---------------------------------------------------------------------------- %
% Start the document
%
\begin{document}

\maketitle

% ------------------------------------------------------------------------ %
% An Abstract is required for SAND reports
%

%
\begin{abstract}
%

MOOCHO (Multifunctional Object-Oriented arCHitecture for Optimization) is a
object-oriented C++ Trilinos package for solving equality and inequality
constrained nonlinear programs (NLPs) using large-scale gradient-based
optimization methods.  The primary focus of MOOCHO up to this point has been
the development of active-set successive quadratic programming (SQP) methods.
MOOCHO was initially developed (under the name rSQP++) to support primarily
reduced-space SQP (rSQP) but other related types of optimization algorithms
can also be developed.  Using MOOCHO, it is possible to specialize all of the
linear-algebra computations and also modify many other parts of the algorithm
externally (without modifying default library source code).  One feature of
the MOOCHO framework is that it supports completely abstract linear algebra
which allows sophisticated implementations on parallel distributed-memory
supercomputers but is not tied to any particular linear algebra library
(although adapters to a few linear algebra libraries are available).  In
addition, MOOCHO contains adapters to support massively parallel
simulation-constrained optimization through Thyra Model Evaluator interface.
Access to a great deal of linear solver technology in Trilinos is available
through the Stratimikos package.

This document provides a high-level overview of MOOCHO that describes the
motivation for MOOCHO, the basic mathematical notation used in MOOCHO, the
algorithms that MOOCHO implements, and what types of optimization problems are
appropriate to be solved by MOOCHO.  More detailed documentation on how to
install MOOCHO, how to define NLPs, and how to run MOOCHO algorithms is
provided in the companion MOOCHO Reference Manual {}\cite{ref:moochorefguide}.

%
\end{abstract}
%


% ------------------------------------------------------------------------ %
% An Acknowledgment section is optional but important
%
\clearpage
\section*{Acknowledgment}

I would like to thank by Ph.D. adviser Dr. Larry Biegler for is help in
teaching me optimization and helping to formulate MOOCHO back when it was
called rSQP++.  I would also like to thank Carl Laird (now Dr. Laird) for
helping on many different aspects of the code and the algorithms.

% ------------------------------------------------------------------------ %
% The table of contents and list of figures and tables
%
\cleardoublepage		% TOC needs to start on an odd page
\tableofcontents
\listoffigures
%\listoftables

% ---------------------------------------------------------------------- %
% An optional preface or Foreword
%\clearpage
%\section*{Preface}
%\addcontentsline{toc}{chapter}{Preface}


% ---------------------------------------------------------------------- %
% An optional executive summary
%\clearpage
%\section*{Summary}
%\addcontentsline{toc}{chapter}{Summary}


% ---------------------------------------------------------------------- %
% An optional glossary. We don't want it to be numbered
%\clearpage
%\section*{Nomenclature}
%\addcontentsline{toc}{chapter}{Nomenclature}
%\begin{description}
%    \item[Term 1]
%        Description
%    \item[Term 2]
%        Description
%    \item[Term 3]
%        Description
%\end{description}


% ---------------------------------------------------------------------- %
% This is where the body of the report begins; usually with an Introduction
%
\SANDmain		% Start the main part of the report

%
\section{Introduction}
%

MOOCHO is an object-oriented C++ software package which implements
gradient-based algorithms for large-scale nonlinear programing.  MOOCHO is
designed to allow the incorporation of many different algorithms and to allow
external configuration of specialized linear-algebra objects such as vectors,
matrices and linear solvers (i.e.\ through Thyra).  Data-structure
independence has been recognized as an important feature missing in many
optimization solvers {}\cite{ref:wright_1999}.

While the MOOCHO framework can be used to implement many different types of
optimization methods (e.g.\ Generalized Reduced Gradient (GR)
{}\cite{ref:engineering_optimization}, Augmented Lagrangian (AL)
{}\cite{JNocedal_SJWright_1999}, Successive Quadratic Programming (SQP)
{}\cite{JNocedal_SJWright_1999}) the main focus has been SQP methods.
Successive quadratic programming (SQP) and related methods are attractive
mainly because they generally require the fewest number of function and
gradient evaluations to solve a problem as compared to other optimization
methods {}\cite{ref:schmid_accel_1993}.  Another attractive property of SQP
methods is that they can be adapted to effectively exploit the structure of
the underlying NLP {}\cite{ref:varvarezos_1994}.  A variation of SQP, known as
reduced-space SQP (rSQP), works well for NLPs where there are few degrees of
freedom (see Section {}\ref{moocho:sec:nlp_formulation}) and many constraints.
Quasi-Newton methods for approximating the reduced Hessian of the Lagrangian
are also very efficient for NLPs with few degrees of freedom.  Another
advantage of rSQP is that a decomposition for the equality constraints can be
used which only requires solves with a basis of the Jacobian of the
constraints (see Section {}\ref{moocho:sec:rSQP}) and therefore can utilize
very specialized application-specific data structures and linear solvers.
Therefore, rSQP methods can be tailored to effectively exploit the structure
of simulation-constrained optimization problems and can show excellent
parallel algorithmic scalability.

There is a distinction to be made between a user of MOOCHO and a developer of
MOOCHO, though it may it be narrow one in some cases.  Here we define a user
as anyone who uses MOOCHO to solve an optimization problem using a pre-written
MOOCHO algorithm.  A MOOCHO user can vary from someone who uses a
pre-developed interface to a modeling environment like AMPL
{}\cite{ref:ampl_1993} to someone who uses MOOCHO to solve a discretized
simulation-constrained optimization problem on a massively parallel computer
using specialized application-specific data structures and linear solvers
{}\cite{ref:biros_1999}.  While the first type of user does not need to write
any C++ code and does not even need to know what C++ is, the latter type of
sophisticated user has to write a fair amount of C++ code.  There are also
many different types of use cases of MOOCHO that lie in between these two
extremes.

In the next section (Section {}\ref{moocho:sec:sqp_background}), the basic
mathematical structure of SQP methods is presented.  This presentation is
intended to establish the nomenclature of MOOCHO for users and developers and
to describe what algorithms MOOCHO actually implements.  The nomenclature that
is established is key to being able to understand the output from the MOOCHO
algorithms.  Appendix {}\ref{app:moocho_nomenclature_summary} contains a
summary of this notation.  The basic software design of MOOCHO is then
described in Section {}\ref{moocho:sec:basic_software_design}.  This is
followed in Section {}\ref{moocho:sec:nlp_and_lin_alg_itfc} by a basic
description of the linear algebra and NLP interfaces in MOOCHO.  These
interfaces provide the foundation for allowing the types of specialized data
structures and linear solvers that an advanced user would use with MOOCHO.
Section {}\ref{moocho:sec:defining_opt_problems} briefly discusses how one can
define an NLP for MOOCHO to solve and where to look for more information.
Section {}\ref{moocho:sec:basic_algo_properties} deals with the basics of
using MOOCHO to solve optimization problems and describes some of the common
properties shared by all MOOCHO algorithms.

%
\section{Mathematical Background}
\label{moocho:sec:sqp_background}
%

%
\subsection{Nonlinear Program (NLP) Formulation}
\label{moocho:sec:nlp_formulation}
%

MOOCHO can be used to solve NLPs of the general form: 

{\bsinglespace
\begin{eqnarray}
\mbox{min}  &  & f(x)                     \label{moocho:eqn:nlp:obj} \\
\mbox{s.t.} &  & c(x) = 0                 \label{moocho:eqn:nlp:equ} \\
            &  & x_L \leq x    \leq x_U   \label{moocho:eqn:nlp:bnds}
\end{eqnarray}
\begin{tabbing}
\hspace{4ex}where:\hspace{5ex}\= \\
\>	$x, x_L, x_U \:\in\:\mathcal{X}$, \\
\>	$f(x) : \:\mathcal{X} \rightarrow \RE$, \\
\>	$c(x) : \:\mathcal{X} \rightarrow \mathcal{C}$, \\
\>	$\mathcal{X} \:\subseteq\:\RE\:^n$, \\
\>	$\mathcal{C} \:\subseteq\:\RE\:^m$.
\end{tabbing}
\esinglespace}

Above, we have been very careful to define vector spaces for the relevant
vectors and nonlinear operators.  In general, only vectors from the same
vector space are compatible and can participate in linear-algebra operations.
Mathematically, the only requirement for the compatibility of real-valued
vector spaces should be that the dimensions match up and that the same inner
products are used.  However, having the same dimension and the same inner
product will not always be sufficient to allow for the compatibility of
vectors from different vector spaces in the implementation (e.g.\ coefficients
of parallel vectors can have different distributions to processes).  Vector
spaces become important later when the NLP interfaces and the implementation
of MOOCHO is discussed in more detail in Section
{}\ref{moocho:sec:nlp_and_lin_alg_itfc}.

We assume that $f(x)$ and $c_j(x)$ for $j = 1 \ldots m$ in
(\ref{moocho:eqn:nlp:obj})--(\ref{moocho:eqn:nlp:equ}) are nonlinear functions
with at least second-order continuous derivatives.  The rSQP algorithms
described later only require first-order information (derivatives) for $f(x)$
and $c_j(x)$.  However, these first derivatives can be provided by
(directional) finite differences if missing.  The simple bound inequality
constraints in (\ref{moocho:eqn:nlp:bnds}) may have lower bounds equal to
$-\infty$ and/or upper bounds equal to $+\infty$.  The absences of some of
these bounds can be exploited by many of the algorithms.

It is very desirable for the functions $f(x)$ and $c(x)$ to at least be
defined (i.e.\ no {}\texttt{NaN} or {}\texttt{Inf} return values) everywhere
in the set defined by the relaxed variable bounds $x_L - \delta \leq x \leq
x_U +
{}\delta$.  Here, $\delta$ (see the method
{}\texttt{max\_var\_bounds\_viol()} in the Doxygen documentation for the
{}\texttt{\textit{NLP}} interface) is a relaxation (i.e.\ wiggle room)
that the user can set to allow the optimization algorithm to compute
$f(x)$ and $c(x)$ outside the strict variable bounds $x_L \le x \le
x_U$ in order to compute finite differences and the like.  The SQP
algorithms in MOOCHO will never evaluate $f(x)$ and $c(x)$ outside the
above relaxed variable bounds.  This gives users a measure of control
in how the optimization algorithms interact with the NLP model.

The Lagrangian function $L(\lambda, \nu_L, \nu_U)$ and the Lagrange
multipliers ($\lambda$, $\nu_L$, $\nu_U$) for this NLP are defined by

{\bsinglespace
\begin{eqnarray}
L(x,\lambda,\nu_L,\nu_U)
& = & f(x) + \lambda^T c(x) + \nu_L^T ( x_L - x ) + \nu_U^T ( x - x_U ) \; \:\in\:\RE,
\label{moocho:eqn:L_def} \\
\nabla_{x} L(x,\lambda,\nu)
& = & \nabla f(x) + \nabla c(x) \lambda + \nu \; \:\in\:\mathcal{X},
\label{moocho:eqn:GL_def} \\
\nabla_{xx}^2 L(x,\lambda)
& = & \nabla^2 f(x) + \sum^m_{j=1} \lambda_{(j)} \nabla^2 c_j(x) \; \:\in\: \mathcal{X}|\mathcal{X},
\label{moocho:eqn:HL_def}
\end{eqnarray}}
%
\begin{tabbing}
\hspace{4ex}where:\hspace{5ex}\= \\
\>	$\nabla f(x) : \:\mathcal{X} \rightarrow \mathcal{X}$, \\
\>	$\nabla c(x) = {\bmat{cccc} \nabla c_1 (x) & \nabla c_2 (x) & \ldots & \nabla c_m (x)  \emat},
         : \:\mathcal{X} \rightarrow \mathcal{X}|\mathcal{C}$, \\
\>	$\nabla^2 f(x) : \:\mathcal{X} \rightarrow \mathcal{X}|\mathcal{X}$, \\
\>	$\nabla^2 c_j(x) : \:\mathcal{X} \rightarrow \mathcal{X}|\mathcal{X} \; \mbox{, for}\:j = 1 \ldots m$, \\
\>	$\lambda \:\in\:\mathcal{C}$, \\
\>	$\nu \equiv \nu_U - \nu_L \:\in\:\mathcal{X}$.
\end{tabbing}

Above, we use the notation $\lambda_{(j)}$ with the subscript in parentheses
to denote the one-based $j^{\mbox{th}}$ component of the vector $\lambda$ and
to differentiate this from a simple math accent.  Also, $\nabla c(x) :
{}\mathcal{X} {}\rightarrow {}\mathcal{X}|\mathcal{C}$ is used to denote a
nonlinear operator (the gradient of the equality constraints $\nabla c(x)$ in
this case) that maps from the vector space $\mathcal{X}$ to a linear-operator
space $\mathcal{X}|\mathcal{C}$ where the range and the domain are the vector
spaces $\mathcal{X}$ and $\mathcal{C}$ respectively.  The returned object $A =
{}\nabla c {}\in\mathcal{X}|\mathcal{C}$ defines a linear operator where $q =
A p$ maps vectors from $p \in\mathcal{C}$ to $q {}\in\mathcal{X}$.  The
transposed object $A^T$ defines an adjoint linear operator where $q = A^T p$
maps vectors from $p {}\in\mathcal{X}$ to $q {}\in\mathcal{C}$.

Given the definition of the Lagrangian and its derivatives in
(\ref{moocho:eqn:L_def})--(\ref{moocho:eqn:HL_def}), the first- and
second-order necessary KKT optimality conditions {}\cite{ref:nash_sofer_1996}
for a solution $(x^*, \lambda^*, \nu^*_L, \nu^*_U)$ to
(\ref{moocho:eqn:nlp:obj})--(\ref{moocho:eqn:nlp:bnds}) are given in
(\ref{moocho:eqn:kkt:lin_dep_grads})--(\ref{moocho:eqn:kkt:HL_psd}).  There
are four different categories of optimality conditions: linear dependence of
gradients (\ref{moocho:eqn:kkt:lin_dep_grads}), feasibility
(\ref{moocho:eqn:kkt:equ_feas})--(\ref{moocho:eqn:kkt:bnds_feas}),
non-negativity of Lagrange multipliers for inequalities
(\ref{moocho:eqn:kkt:nonneg_bnds_mult}), complementarity
(\ref{moocho:eqn:kkt:xl_comp})--(\ref{moocho:eqn:kkt:xu_comp}), and curvature
(\ref{moocho:eqn:kkt:HL_psd}).

{\bsinglespace
\begin{equation}
\nabla_{x} L(x^*,\lambda^*,\nu^*) = \nabla f(x^*) + \nabla c(x^*) \lambda^* + \nu^* = 0
\label{moocho:eqn:kkt:lin_dep_grads}
\end{equation}
%
\begin{equation}
c(x^*) = 0
\label{moocho:eqn:kkt:equ_feas}
\end{equation}
%
\begin{equation}
x_L \leq x^* \leq x_U
\label{moocho:eqn:kkt:bnds_feas}
\end{equation}
%
\begin{equation}
(\nu_L)^*, (\nu_U)^* \geq 0
\label{moocho:eqn:kkt:nonneg_bnds_mult}
\end{equation}
%
\begin{equation}
(\nu_L)^*_{(i)} ( (x_L)_{(i)} - (x^*)_{(i)} ) = 0, \;\; \mbox{for} \; i = 1 \ldots n
\label{moocho:eqn:kkt:xl_comp}
\end{equation}
%
\begin{equation}
(\nu_U)^*_{(i)} ( (x^*)_{(i)} - (x_U)_{(i)} ) = 0, \;\; \mbox{for} \; i = 1 \ldots n
\label{moocho:eqn:kkt:xu_comp}
\end{equation}
%
\begin{equation}
d^T \: \nabla_{xx}^2 L(x^*,\lambda^*) \: d \geq 0, \;\; \mbox{for all feasible directions $d \:\in\:\mathcal{X}$}.
\label{moocho:eqn:kkt:HL_psd}
\end{equation}
\esinglespace}

Sufficient conditions for optimality require that stronger assumptions be made
about the NLP (e.g.\ a constraint qualification on $c(x)$ and perhaps
conditions on third-order curvature in case
%
\[
d^T \: \nabla_{xx}^2 L(x^*,\lambda^*) \: d = 0
\]
%
for $d {}\ne 0$ in (\ref{moocho:eqn:kkt:HL_psd}).

To solve an NLP, an SQP algorithm must first be supplied an initial guess for
the unknown variables $x_0$ and in some cases also initial guesses for the
Lagrange multipliers $\lambda_0$ and $\nu_0$.  The optimization algorithms
implemented in MOOCHO generally require that the initial guess $x_0$ satisfy
the variable bounds in (\ref{moocho:eqn:nlp:bnds}), and if not, then the
elements of $x_0$ can be forced in bounds before starting the algorithm.

%
\subsection{Successive Quadratic Programming (SQP)}
\label{moocho:sec:SQP}
%

A popular class of methods for solving NLPs is successive quadratic
programming (SQP) {}\cite{ref:boggs_tolle_1996}.  An SQP method is
equivalent, in many cases, to applying Newton's method to solve the
optimality conditions represented by
(\ref{moocho:eqn:kkt:lin_dep_grads})--(\ref{moocho:eqn:kkt:equ_feas}).
At each Newton iteration $k$ for
(\ref{moocho:eqn:kkt:lin_dep_grads})--(\ref{moocho:eqn:kkt:equ_feas}),
the linear subproblem (also known as the KKT system) takes the form

{\bsinglespace
\begin{equation}
{\bmat{cc}
	W    & A \\
	A^T  &
\emat}
{\bmat{c}
	d \\
	d_{\lambda}
\emat}
=
-
{\bmat{c}
	\nabla_x L \\
	c
\emat}
\label{moocho:eqn:full_kkt_sys}
\end{equation}
\begin{tabbing}
\hspace{4ex}where:\hspace{5ex}\= \\
\>	$d = x_{k+1} - x_k \:\in\:\mathcal{X}$, \\
\>	$d_{\lambda} = \lambda_{k+1} - \lambda_k \:\in\:\mathcal{C}$, \\
\>	$W = \nabla_{xx}^2 L(x_k,\lambda_k) \:\in\:\mathcal{X}|\mathcal{X}$, \\
\>	$A = \nabla c(x_k) \:\in\:\mathcal{X}|\mathcal{C}$, \\
\>	$c = c(x_k) \:\in\:\mathcal{C}$.
\end{tabbing}
\esinglespace}

The Newton matrix in (\ref{moocho:eqn:full_kkt_sys}) is known as the KKT
matrix.  By substituting $d_{\lambda} = {}\lambda_{k+1} - {}\lambda_k$ into
(\ref{moocho:eqn:full_kkt_sys}) and moving ${}\lambda_k$ to the
right-hand-side, this linear system becomes equivalent to the optimality
conditions of the following QP.

{\bsinglespace
\begin{eqnarray}
\mbox{min}  &  & g^T d + \myonehalf d^T W d   \label{moocho:eqn:qp_newton:obj} \\
\mbox{s.t.} &  & A^T d + c = 0                \label{moocho:eqn:qp_newton:equ}
\end{eqnarray}
\begin{tabbing}
\hspace{4ex}where:\hspace{5ex}\= \\
\>	$g = \nabla f(x_k) \:\in\:\mathcal{X}.$
\end{tabbing}
\esinglespace}

The advantage of the QP formulation over the Newton linear system formulation
is that inequality constraints can be directly added to the QP and a
relaxation can be defined to allow for infeasible subproblems, which yields
the following QP.

{\bsinglespace
\begin{eqnarray}
\mbox{min}  &  & g^T d + \myonehalf d^T W d + M(\eta)                   \label{moocho:eqn:qp:obj} \\
\mbox{s.t.} &  & A^T d + (1-\eta) c = 0                                 \label{moocho:eqn:qp:equ} \\
            &  & x_L - x_k \leq d \leq x_U -x_k                         \label{moocho:eqn:qp:bnds} \\
            &  & 0 \leq \eta \leq 1                                     \label{moocho:eqn:qp:eta_bnd}
\end{eqnarray}
\begin{tabbing}
\hspace{4ex}where:\hspace{5ex}\= \\
\>	$M(\eta) \:\in\:\RE \rightarrow \RE$.
\end{tabbing}
\esinglespace}

Near the solution of the NLP, the set of optimal active constraints for
(\ref{moocho:eqn:qp:obj})--(\ref{moocho:eqn:qp:eta_bnd}) will be the same as
the optimal active-set for the NLP in
(\ref{moocho:eqn:nlp:obj})--(\ref{moocho:eqn:nlp:bnds}) {}\cite[Theorem
18.1]{JNocedal_SJWright_1999}.

The relaxation of the QP shown in
(\ref{moocho:eqn:qp:obj})--(\ref{moocho:eqn:qp:eta_bnd}) is only one form of a
relaxation but has some essential properties.  For example, the solution $\eta
= 1$ and $d = 0$ is always feasible by construction.  However, the solution
$\eta = 1$ and $d = 0$ is of little practical use since it results in zero
steps.  The penalty function $M(\eta)$ is either linear or quadratic where if
$\frac{\partial M(\eta)}{\partial {}\eta}|_{\eta = 0}$ is sufficiently large
then an unrelaxed solution (i.e.\ $\eta = 0$) will be obtained if a feasible
region for the original QP exists.  For example, the penalty term may take a
form such as $M(\eta) = {}\eta \tilde{M}$ or $M(\eta) = (\eta + {}\myonehalf
{}\eta^2)\tilde{M}$ where $\tilde{M}$ is a large constant often called ``big
M''.  The default QP solver in MOOCHO, QPSchur {}\cite{bartlett-qpschur}, is
careful not to allow the ill-conditioning associated with $\tilde{M}$ to
impact the solution unless it is needed for an infeasible QP.

Once a new estimate of the solution ($x_{k+1}$, $\lambda_{k+1}$, $\nu_{k+1}$)
is computed, the error in the optimality conditions
(\ref{moocho:eqn:kkt:lin_dep_grads})--(\ref{moocho:eqn:kkt:bnds_feas}) is
checked.  If these KKT errors are within some specified tolerance, the
algorithm is terminated with the optimal solution.  If the KKT error is too
large, the NLP functions and gradients are then computed at the new point
$x_{k+1}$ and another QP subproblem
(\ref{moocho:eqn:qp:obj})--(\ref{moocho:eqn:qp:eta_bnd}) is solved which
generates another step $d$ and so on.  This algorithm is continued until a
solution is found or the algorithm runs into trouble (there can be many causes
for algorithm failure), or it is prematurely terminated because it is taking
too long (i.e.\ the maximum number of iterations or maximum runtime is
exceeded).

The iterates generated from $x_{k+1} = x_k + d$ are generally only guaranteed
to converge to a local minimum to the first-order KKT conditions when close to
the solution.  Therefore, globalization methods are used to insure (given a
few, sometimes strong, assumptions are satisfied) the SQP algorithm will
converge to a local minimum from remote starting points.  One popular class of
globalization methods are line search methods.  In a line search method, once
the step $d$ is computed from the QP subproblem, a line search procedure is
used to find a step length $\alpha$ such that $x_{k+1} = x_k + {}\alpha d$
gives {\em sufficient reduction} in the value of a {\em merit function}
$\phi(x_{k+1}) < \phi(x_k)$.  A merit function is used to balance a trade-off
between minimizing the objective function $f(x)$ and reducing the error in the
constraints $c(x)$.  A commonly used merit function is the $\ell_1$
(\ref{moocho:eqn:phi_L1}) where $\mu$ is a penalty parameter that is adjusted
to insure descent along the SQP step $x_k + \alpha d$ for $\alpha > 0$.

{\bsinglespace
\begin{equation}
\phi_{\ell_1}(x) = f(x) + \mu ||c(x)||_1
\label{moocho:eqn:phi_L1}
\end{equation}
\esinglespace}

An alternative line search based on a ``Filter'' has also been implemented
which generally performs better and does not require the maintenance of a
penalty parameter $\mu$.  Other globalization methods such as trust region
(using a merit function or the filter) can also be applied to SQP but no trust
region method is currently implemented in MOOCHO.

Because SQP is essentially equivalent to applying Newton's method to the
optimality conditions, it can be shown to be quadratically convergent near the
solution of the NLP {}\cite{ref:nocedal_overton_1985}.  It is this fast rate
of convergence that makes SQP the method of choice for many applications.
However, there are many theoretical and practical details that need to be
considered.  One difficulty is that in order to achieve quadratic convergence
the exact Hessian of the Lagrangian $W$ is needed, which requires exact
second-order information $\nabla^2 f(x)$ and $\nabla^2 c_j(x)$, $j = 1 \ldots
m$.  For many NLP applications, second derivatives are not readily available
and it is too expensive and/or inaccurate to compute them using finite
differences.  Other difficulties with SQP include how to deal with an
indefinite projected Hessian.  Also, for large problems, the full QP
subproblem in (\ref{moocho:eqn:qp:obj})--(\ref{moocho:eqn:qp:eta_bnd}) can be
extremely expensive to solve directly.  These and other difficulties have
motivated the research of large-scale decomposition methods for SQP.  One
class of these methods is reduced-space (or reduced Hessian) SQP, or rSQP for
short.  RSQP methods are discussed in more detail in the next section.

%
\subsection{Reduced-Space Successive Quadratic Programming (rSQP)}
\label{moocho:sec:rSQP}
%

In a reduced-space SQP (rSQP) method, the full-space QP subproblem
(\ref{moocho:eqn:qp:obj})--(\ref{moocho:eqn:qp:eta_bnd}) is decomposed into
two smaller subproblems that, in many cases, are easier to solve.  To see how
this is done, first a null-space decomposition {}\cite[Section
18.3]{ref:nocedal_wright_1999} is computed for some linearly independent set
of the linearized equality constraints $A_d \:\in\:\mathcal{X}|\mathcal{C}_d$
where $c_d(x)\:\in\:\mathcal{C}_d\:\in\:\RE\:^{r}$ are the decomposed equality
constraints and $c_u(x)\:\in\:\mathcal{C}_u\:\in\:\RE\:^{(m-r)}$ are the
undecomposed equality constraints and

{\bsinglespace
\begin{equation}
c(x) =
{\bmat{c} c_d(x) \\ c_u(x) \emat} \:\in\:\mathcal{C}_d \times \mathcal{C}_u
\; \Longrightarrow \;
\nabla c(x_k) = {\bmat{cc} \nabla c_d(x_k) & \nabla c_u(x_k) \emat}
= {\bmat{cc} A_d & A_u \emat} \:\in\:\mathcal{X}|(\mathcal{C}_d \times \mathcal{C}_u).
\label{moocho:eqn:lin_indep_constr}
\end{equation}
\esinglespace}
%
Above, the vector space $\mathcal{C} = \mathcal{C}_d \times \mathcal{C}_u$
denotes a blocked vector space (also known as a product space) with a
dimension which is the sum of the constituent vector spaces $|\mathcal{C}| =
|\mathcal{C}_d| + |\mathcal{C}_u| = r + (m - r) = m$.  This decomposition is
defined by a null-space linear operator $Z$ and a linear operator $Y$ with the following
properties:

{\bsinglespace
\begin{equation}
\begin{array}{ll}
Z \:\in\:\mathcal{X}|\mathcal{Z}
	& \mbox{s.t.} \; (A_d)^T Z = 0 \\
Y \:\in\:\mathcal{X}|\mathcal{Y}
	& \mbox{s.t.} \; {\bmat{cc} Y & Z \emat} \; \mbox{is nonsingular}
\end{array}
\label{moocho:eqn:Z_Y_def}
\end{equation}
\begin{tabbing}
\hspace{4ex}where:\hspace{5ex}\= \\
\>	$\mathcal{Z} \:\subseteq\:\RE\:^{(n-r)}$ \\
\>	$\; \mathcal{Y} \:\subseteq\:\RE\:^{r}$.
\end{tabbing}
\esinglespace}
%
It is important to distinguish the vector spaces $\mathcal{Z}$ and
$\mathcal{Y}$ from the the linear operators $Z$ and $Y$.  The null-space
linear operator $Z\:\in\:\mathcal{X}|\mathcal{Z}$ is a linear operator that
maps vectors from the null-space space $u\:\in\:\mathcal{Z}$ to vectors in the
space of the unknowns $v = Z u \:\in\:\mathcal{X}$.  The linear operator
$Y\:\in\:\mathcal{X}|\mathcal{Y}$ is a linear operator that maps vectors from
the space $u\:\in\:\mathcal{Y}$ to vectors in the space of the unknowns $v = Y
u \:\in\:\mathcal{X}$.

In many presentations of reduced-space SQP, the linear operator $Y$ is
referred to as the ``range-space'' linear operator since several popular
choices of this linear operator form a basis for the range space of $A_d$.
However, note that the linear operator $Y$ need not be a true basis linear
operator for the range-space of $A_d$ in order to satisfy the non-singularity
property in (\ref{moocho:eqn:Z_Y_def}).  For this reason, here the linear
operator $Y$ will be referred to as the ``quasi-range-space'' linear operator
to make this distinction.

By using (\ref{moocho:eqn:Z_Y_def}), the search direction $d$ can be broken
down into $d = (1-\eta) Y p_y + Z p_z$, where $p_y \:\in\:\mathcal{Y}$ and
$p_z \:\in\:\mathcal{Z}$ are the known as the quasi-normal (or quasi-range
space) and tangential (or null space) steps respectively.  By substituting $d
= (1-\eta) Y p_y + Z p_z$ into
(\ref{moocho:eqn:qp:obj})--(\ref{moocho:eqn:qp:eta_bnd}) we obtain the
quasi-normal (\ref{moocho:eqn:range_space_step}) and tangential
(\ref{moocho:eqn:moocho:obj})--(\ref{moocho:eqn:moocho:inequ}) subproblems.  In
(\ref{moocho:eqn:moocho:obj}), $\zeta \leq 1 $ is a damping parameter which can
be used to insure descent of the merit function $\phi(x_{k+1}+\alpha
d)$.\\[1ex]

{\bsinglespace
\begin{center}\textbf{Quasi-Normal (Quasi-Range-Space) Subproblem}\end{center}
\begin{equation}
p_y = - R^{-1} c_d \:\in\:\mathcal{Y}
\label{moocho:eqn:range_space_step}
\end{equation}
\hspace{4ex}where: $R \equiv [(A_d)^T Y]  \:\in\:\mathcal{C}_d|\mathcal{Y}$
	(nonsingular via (\ref{moocho:eqn:Z_Y_def})). \\[2ex]

\begin{center}\textbf{Tangential (Null-Space) Subproblem (Relaxed)}\end{center}
\begin{eqnarray}
\mbox{min}  &  & (g^r + \zeta w)^T p_z + \myonehalf p_z^T [Z^T W Z] p_z + M(\eta)
                 \label{moocho:eqn:moocho:obj} \\
\mbox{s.t.} &  & U_z p_z + (1-\eta) u = 0 
                 \label{moocho:eqn:moocho:equ} \\
            &  & b_L \leq Z p_z - (Y p_y) \eta \leq b_U	
                 \label{moocho:eqn:moocho:inequ}
\end{eqnarray}
\begin{tabbing}
\hspace{4ex}where:\hspace{5ex}\= \\
\>	$g^r \equiv Z^T g \:\in\:\mathcal{Z}$ \\
\>	$w \equiv Z^T W Y p_y \:\in\:\mathcal{Z}$ \\
\>	$\zeta \:\in\:\RE$ \\
\>	$U_z \equiv [(A_u)^T Z] \:\in\:\mathcal{C}_u|\mathcal{Z}$ \\
\>	$U_y \equiv [(A_u)^T Y] \:\in\:\mathcal{C}_u|\mathcal{Y}$ \\
\>  $u   \equiv U_y p_y + c_u \:\in\:\mathcal{C}_u$ \\
\>	$b_L \equiv x_L - x_k - Y p_y \:\in\:\mathcal{X}$ \\
\>	$b_U \equiv x_U - x_k - Y p_y \:\in\:\mathcal{X}$.
\end{tabbing}
\esinglespace}

By using this decomposition, the Lagrange multipliers $\lambda_d$ for the
decomposed equality constraints ($(A_d)^T d + c_d = 0$) do not need to be
computed in order to produce steps $d = (1-\eta) Y p_y + Z p_z$.  However,
these multipliers can be used to determine the penalty parameter $\mu$ for the
merit function {}\cite[page 544]{ref:nocedal_wright_1999} or to compute the
Lagrangian function.  Alternatively, a multiplier free method for computing
$\mu$ has been developed and tested with good results
{}\cite{ref:schmid_rsqp_1994}.  In any case, it is useful to compute these
multipliers at the solution of the NLP since they give the sensitivity of the
objective function to those constraints {}\cite[page
436]{ref:nash_sofer_1996}.  An expression for computing $\lambda_d$ can be
derived by applying (\ref{moocho:eqn:Z_Y_def}) to $Y^T \nabla
L(x,\lambda,\nu)=0$ to yield

{\bsinglespace
\begin{equation}
\lambda_d = - R^{-T} \left( Y^T(g + \nu) + U_y^T \lambda_u \right)
    \:\in\:\mathcal{C}_d.
\label{moocho:eqn:lambda_d}
\end{equation}
\esinglespace}

There are many details that need to be worked out in order to implement an
rSQP algorithm and there are opportunities for a lot of variability.  There
are some significant decisions that need to be made such as how to compute the
null-space decomposition that defines the matrices $Z$, $Y$, $R$, $U_z$ and
$U_y$; and how the reduced Hessian $Z^T W Z$ and the cross term $w$ in
(\ref{moocho:eqn:moocho:obj}) are calculated (or approximated).

There are several different ways to compute decomposition matrices $Z$ and $Y$
that satisfy (\ref{moocho:eqn:Z_Y_def}) {}\cite{ref:schmid_accel_1993}.  For
small-scale rSQP, an orthonormal $Z$ and $Y$ ($Z^T Y = 0$, $Z^T Z = I$, $Y^T Y
= I$) can be computed using a QR factorization of $A_d$
{}\cite{ref:nocedal_overton_1985}.  This decomposition gives rise to rSQP
algorithms with many desirable properties.  However, using a QR factorization
when $A_d$ is of very large dimension is prohibitively expensive.  MOOCHO
currently does not implement a orthonormal QR decomposition but one can be
added at some point if needed.  Other choices for $Z$ and $Y$ have been
investigated that are more appropriate for large-scale rSQP.  Methods that are
more computationally tractable are based on a variable-reduction decomposition
{}\cite{ref:schmid_accel_1993}.  In a variable-reduction decomposition, the
variables are partitioned into dependent $x_D$ and independent $x_I$ sets

{\bsinglespace
\begin{eqnarray}
x_D & & \:\in\:\mathcal{X}_D \\
x_I & & \:\in\:\mathcal{X}_I \\
x = {\bmat{c} x_D \\ x_I \emat} & & \:\in\: \mathcal{X}_D \times \mathcal{X}_I
\label{moocho:eqn:x_D_I} \\
\end{eqnarray}
\begin{tabbing}
\hspace{4ex}where:\hspace{5ex}\= \\
\>	$\mathcal{X}_D\:\subseteq\:\RE^{r}$\\
\>	$\mathcal{X}_I\:\subseteq\:\RE^{n-r}$
\end{tabbing}
\esinglespace}

\noindent{}such that the Jacobian of the constraints $A^T$ is partitioned as
shown in (\ref{moocho:eqn:basis_partitioning}) where $C$ is a square,
nonsingular linear operator known as the basis matrix.  The variables $x_D$
and $x_I$ are also called the state and design (or controls) variables
{}\cite{GBiros_OGhattas_1999a} in some contexts or the basic and nonbasic
variables {}\cite{ref:murtagh_minos_1995} in other contexts.  What is
important about this partitioning of variables is that the $x_D$ variables
define the selection of the basis matrix $C$, nothing more.  Some types of
optimization algorithms give more significance to this partitioning of
variables (for example, in MINOS {}\cite{ref:murtagh_minos_1995} the basic
variables are also variables that are not at an active bound) however no extra
significance can be attributed here.

This basis selection is used to define a variable-reduction null-space matrix
$Z$ in (\ref{moocho:eqn:vr:Z}) which also determines $U_z$ in
(\ref{moocho:eqn:vr:Uz}).

{\bsinglespace
\begin{center}\textbf{Variable-Reduction Partitioning}\end{center}
\begin{equation}
A^T =
{\bmat{c}
(A_d)^T \\
(A_u)^T
\emat}
=
{\bmat{cc}
C & N \\
E & F
\emat}
\label{moocho:eqn:basis_partitioning}
\end{equation} 
\begin{tabbing}
\hspace{4ex}where:\hspace{5ex}\= \\
\>	$C \:\in\:\mathcal{C}_d|\mathcal{X}_D$ \hspace{4ex} (nonsingular)\\
\>	$N \:\in\:\mathcal{C}_d|\mathcal{X}_I$ \\
\>	$E \:\in\:\mathcal{C}_u|\mathcal{X}_D$ \\
\>	$F \:\in\:\mathcal{C}_u|\mathcal{X}_I$.
\end{tabbing}

\begin{center}\textbf{Variable-Reduction Null-Space Matrix}\end{center}
\begin{eqnarray}
Z & \equiv & {\bmat{c} - C^{-1} N \\ I \emat}       \label{moocho:eqn:vr:Z}  \\
U_z & = & F - E \: C^{-1} N                         \label{moocho:eqn:vr:Uz} 
\end{eqnarray}
\esinglespace}

There are many choices for the quasi-range-space matrix $Y$ that
satisfy (\ref{moocho:eqn:Z_Y_def}).  Two relatively computationally
inexpensive choices are the coordinate and orthogonal decompositions
shown below.

{\bsinglespace
\begin{center}\textbf{Coordinate Variable-Reduction Null-Space Decomposition}\end{center}
\begin{eqnarray}
Y & \equiv & {\bmat{c} I \\ 0 \emat}    \label{moocho:eqn:vr_coor:Y} \\
R & = & C                               \label{moocho:eqn:vr_coor:R} \\
U_y & = & E                             \label{moocho:eqn:vr_coor:Uy}
\end{eqnarray}

\begin{center}\textbf{Orthogonal Variable-Reduction Null-Space Decomposition}\end{center}
\begin{eqnarray}
Y & \equiv & {\bmat{c} I \\ N^T C^{-T} \emat}       \label{moocho:eqn:vr_ortho:Y} \\
R & = & C (I + C^{-1 }N N^T C^{-T})                 \label{moocho:eqn:vr_ortho:R} \\
U_y & = & E - F N^T C^{-T}                          \label{moocho:eqn:vr_ortho:Uy}
\end{eqnarray}
\esinglespace}

The orthogonal decomposition ($Z^T Y = 0$, $Z^T Z \neq I$, $Y^T Y \neq I$)
defined in (\ref{moocho:eqn:vr:Z})--(\ref{moocho:eqn:vr:Uz}) and
(\ref{moocho:eqn:vr_ortho:Y})--(\ref{moocho:eqn:vr_ortho:Uy}) is more
numerically stable than the coordinate decomposition defined in
(\ref{moocho:eqn:vr:Z})--(\ref{moocho:eqn:vr:Uz}) and
(\ref{moocho:eqn:vr_coor:Y})--(\ref{moocho:eqn:vr_coor:Uy}) and has other
desirable properties in the context of rSQP {}\cite{ref:schmid_accel_1993}.

Solves with $R$ in (\ref{moocho:eqn:vr_ortho:R}) are performed using the
Sherman-Morrison-Woodbury formula {}\cite{JNocedal_SJWright_1999} which gives
%
\begin{equation}
R^{-1} = ( I - D \: S^{-1} \: D^T ) C^{-1}
\label{moocho:eqn:vr_ortho:R:SMW}
\end{equation}
%
where $D = -C^{-1} N {}\in\mathcal{X}_D|\mathcal{X}_I$ and $S = I + D^T D
{}\in\mathcal{X}_I|\mathcal{X}_I$ are explicitly computed, and the symmetric
positive definite matrix $S$ is factored using a dense Cholesky method.
Therefore, applying $R^{-1}$ only requires a solve with the basis matrix $C$
along with back-solving with the factor of $S$.  However, the $n_I$ linear
solves needed to form $D = -C^{-1} N$ and the $O((n-r)^2 r)$ dense linear
algebra required to compute $D^T D$ can dominate the cost of the algorithm for
NLPs with larger numbers of degrees of freedom $(n-r)$.

For larger $(n-r)$ if adjoint solves with $C^T$ are available, the coordinate
decomposition ($Z^T Y {}\neq 0$, $Z^T Z {}\neq I$, $Y^T Y \neq I$) defined in
(\ref{moocho:eqn:vr:Z})--(\ref{moocho:eqn:vr:Uz}) and
(\ref{moocho:eqn:vr_coor:Y})--(\ref{moocho:eqn:vr_coor:Uy}) is preferred
because it is cheaper but the downside is that it is also more susceptible to
problems associated with a poor selection of dependent variables and
ill-conditioning in the basis matrix $C$ that can result in greatly degraded
performance and even failure of an rSQP algorithm.  See the MOOCHO option
{}\texttt{quasi\-\_range\-\_space\-\_matrix} in Section
{}\ref{moocho:sec:solver_options} for selecting between the orthogonal and the
coordinate decompositions.

It is also important to note that MOOCHO can be used to solve
non-equality-constrained optimization problems ($m=0$) and square nonlinear
equations ($m=n$).  A non-equality-constrained optimization problem is handled
by using $Z=I$ and $Y=\{\mbox{empty}\}$.  A square nonlinear problem is
handled using $Z=\{\mbox{empty}\}$ and $Y=I$.  MOOCHO configures simpler
algorithms in these two cases.

Another important decision is how to compute the reduced Hessian $Z^T W Z$.
For many NLPs, second derivative information is not available to compute the
Hessian of the Lagrangian $W$ directly.  In these cases, first derivative
information can be used to approximate the reduced Hessian $B {}\approx Z^T W
Z$ using quasi-Newton methods (e.g.\ BFGS) {}\cite{ref:nocedal_overton_1985}.
When $(n-r)$ is small, $B$ is small and cheap to update.  Under the proper
conditions the resulting quasi-Newton, rSQP algorithm has a superlinear rate
of local convergence (even using $w$ = 0 in (\ref{moocho:eqn:moocho:obj}))
{}\cite{ref:biegler_et_al_1995}.  When $(n-r)$ is large, limited-memory
quasi-Newton methods can be used, but the price one pays is in only being able
to achieve a linear rate of convergence (with a small rate constant
hopefully).  For some classes of NLPs, good approximations of the Hessian $W$
are available and may have specialized properties (i.e.\ structure) that makes
computing the exact reduced Hessian $B = Z^T W Z$ computationally feasible
(i.e.\ see NMPC in {}\cite{RABartlett_2001}).  See the option
{}\texttt{quasi\_newton} in Section {}\ref{moocho:sec:solver_options}.  Other
options include solving for system with the exact reduced Hessian $B = Z^T W
Z$ iteratively (using CG for instance) which only requires matrix-vector
products with $W$ which can be computed efficiently using automatic
differentiation (for instance) in some cases {}\cite{ref:adolc_1996}.
However, MOOCHO currently does not have any such algorithms implemented at
this time.

In addition to variations that affect the convergence behavior of the
rSQP algorithm, such as null-space decompositions, approximations used
for the reduced Hessian and many different types of merit functions
and globalization methods, there are also many different
implementation options.  For example, linear systems such as
(\ref{moocho:eqn:range_space_step}) can be solved using direct or
iterative solvers and the reduced QP subproblem in
(\ref{moocho:eqn:moocho:obj})--(\ref{moocho:eqn:moocho:inequ}) can be
solved using a variety of methods (active set vs. interior point) and
software {}\cite{ref:schmid_qpkwik_1994}.

%
\subsection{General Inequalities, Slack Variables and Basis Permutations}
\label{moocho:sec:nlp_with_slacks}
%

Up to this point, only simple variable bounds in
(\ref{moocho:eqn:nlp:bnds}) have been considered and the SQP and rSQP
algorithms have been presented in this context.  However, the actual
underlying NLP may include general inequalities and take the form
%
{\bsinglespace
\begin{eqnarray}
\mbox{min}  &  & \breve{f}(\breve{x})                                     \label{moocho:eqn:nlporig:obj} \\
\mbox{s.t.} &  & \breve{c}(\breve{x}) = 0                                 \label{moocho:eqn:nlporig:equ} \\
            &  & \breve{h}_L \leq \breve{h}(\breve{x}) \leq \breve{h}_U   \label{moocho:eqn:nlporig:inequ} \\
            &  & \breve{x}_L \leq \breve{x}            \leq \breve{x}_U   \label{moocho:eqn:nlporig:bnds}
\end{eqnarray}
\begin{tabbing}
\hspace{4ex}where:\hspace{5ex}\= \\
\>	$\breve{x}, \breve{x}_L, \breve{x}_U \:\in\:\breve{\mathcal{X}}$ \\
\>	$\breve{f}(x) : \:\breve{\mathcal{X}} \rightarrow \RE$ \\
\>	$\breve{c}(x) : \:\breve{\mathcal{X}} \rightarrow \breve{\mathcal{C}}$ \\
\>	$\breve{h}(x) : \:\breve{\mathcal{X}} \rightarrow \breve{\mathcal{H}}$ \\
\>	$\breve{h}_L, \breve{h}_L \:\in\:\breve{\mathcal{H}}$ \\
\>	$\breve{\mathcal{X}} \:\in\:\RE\:^{\breve{n}}$ \\
\>	$\breve{\mathcal{C}} \:\in\:\RE\:^{\breve{m}}$ \\
\>	$\breve{\mathcal{H}} \:\in\:\RE\:^{\breve{m}_I}$.
\end{tabbing}
\esinglespace}

NLPs with general inequalities are converted into the standard form by
the addition of slack variables $\breve{s}$ (see
(\ref{moocho:eqn:nlpmap:c})).  After the addition of the slack
variables, the concatenated variables and constraints are then
permuted (using permutation matrices $Q_x$ and $Q_c$) according to the
current basis selection into the ordering in
(\ref{moocho:eqn:nlp:obj})--(\ref{moocho:eqn:nlp:bnds}).  The exact
mapping from
(\ref{moocho:eqn:nlporig:obj})--(\ref{moocho:eqn:nlporig:bnds}) to
(\ref{moocho:eqn:nlp:obj})--(\ref{moocho:eqn:nlp:bnds}) is
%
{\bsinglespace
\begin{eqnarray}
x & = & Q_x {\bmat{c} \breve{x} \\ \breve{s} \emat} \label{moocho:eqn:nlpmap:x} \\
x_L & = & Q_x {\bmat{c} \breve{x}^L \\ \breve{h}^L \emat} \label{moocho:eqn:nlpmap:xl} \\
x_U & = & Q_x {\bmat{c} \breve{x}_u \\ \breve{h}_u \emat} \label{moocho:eqn:nlpmap:xu} \\
c(x) & = & Q_c {\bmat{c} \breve{c}(\breve{x}) \\ \breve{h}(\breve{x}) - \breve{s} \emat}.
\label{moocho:eqn:nlpmap:c}
\end{eqnarray}
\esinglespace}

Here we consider the implications of the above transformation in the context
of rSQP algorithms.

Note if $Q_x = I$ and $Q_c = I$ that the matrix $\nabla c$ takes the form
%
\begin{equation}
\nabla c = {\bmat{cc} \nabla \breve{c} & \nabla \breve{h} \\ & -I \emat}
	\label{moocho:eqn:Gc_orig}
\end{equation}

One question to ask is how the Lagrange multipliers for the original
constraints can be extracted from the optimal solution $(x,\lambda,\nu)$ that
satisfies the optimality conditions in
(\ref{moocho:eqn:kkt:lin_dep_grads})--(\ref{moocho:eqn:kkt:HL_psd})?  First,
consider the linear dependence of gradients optimality condition for the NLP
formulation in (\ref{moocho:eqn:nlporig:obj})--(\ref{moocho:eqn:nlporig:bnds})
%
\begin{equation}
\nabla_{\breve{x}} \breve{L}(\breve{x}^*,\breve{\lambda}^*,\breve{\lambda_I}^*,\breve{\nu}^*)
= \nabla \breve{f}(\breve{x}^*) + \nabla \breve{c}(\breve{x}^*) \breve{\lambda}^*
+ \nabla \breve{h}(\breve{x}^*) \breve{\lambda_I}^* + \breve{\nu}^* = 0.
\label{moocho:eqn:kktorig:lin_dep_grads}
\end{equation}

To see how the Lagrange multiples $\lambda^*$ and $\nu^*$ can be used to
compute $\breve{\lambda}^*$, $\breve{\lambda_I}^*$ and $\breve{\nu}^*$ one
simply has to substitute (\ref{moocho:eqn:nlpmap:x}) and
(\ref{moocho:eqn:nlpmap:c}) with $Q_x = I$ and $Q_c = I$, for instance, into
(\ref{moocho:eqn:kkt:lin_dep_grads}) and expand as follows
%
\begin{eqnarray}
\nabla_{x} L(x,\lambda,\nu)
 & = & \nabla f + \nabla c \lambda + \nu \nonumber \\
 & = & {\bmat{c} \nabla \breve{f} \\ 0 \emat}
     + {\bmat{cc} \nabla \breve{c} & \nabla \breve{h} \\ & -I \emat}
       {\bmat{c} \lambda_{\breve{c}} \\ \lambda_{\breve{h}} \emat}
     + {\bmat{c} \nu_{\breve{x}} \\ \nu_{\breve{s}} \emat}
	\nonumber \\
 & = & {\bmat{c}
	\nabla \breve{f} + \nabla \breve{c} \lambda_{\breve{c}} + \nabla \breve{h} \lambda_{\breve{h}} + \nu_{\breve{x}} \\
	-\lambda_{\breve{h}} + \nu_{\breve{s}}
	\emat}.
	\label{moocho:eqn:kkt:compare_lagr}
\end{eqnarray}
%
By comparing (\ref{moocho:eqn:kktorig:lin_dep_grads}) and
(\ref{moocho:eqn:kkt:compare_lagr}) it is clear that the mapping is
$\breve{\lambda} = \lambda_{\breve{c}}$, $\breve{\lambda}_I =
\lambda_{\breve{h}} = \nu_{\breve{s}}$ and $\breve{\nu} = \nu_{\breve{x}}$.
For arbitrary $Q_x$ and $Q_c$ it is also easy to perform the mapping of the
solution.  What is interesting about (\ref{moocho:eqn:kkt:compare_lagr}) is
that it says that for general inequalities $\breve{h}_j(\breve{x})$ that are
not active at the solution (i.e.\ $(\nu_{\breve{s}})_{(j)} = 0$), the Lagrange
multiplier for the converted equality constraint $(\lambda_{\breve{h}})_{(j)}$
will be zero.  This means that these converted inequalities can be eliminated
from the problem and not impact the solution (which is what one would have
expected).  Zero multiplier values means that constraints will not impact the
optimality conditions or the Hessian of the Lagrangian.

The basis selection shown in (\ref{moocho:eqn:lin_indep_constr}) and
(\ref{moocho:eqn:x_D_I}) is determined by the permutation matrices $Q_x$ and
$Q_c$ and these permutation matrices can be partitioned as
%
\begin{eqnarray}
Q_{x} & = & {\bmat{c} Q_{xD} \\ Q_{xI} \emat} \label{moocho:eqn:Qx} \\
Q_{c} & = & {\bmat{c} Q_{cD} \\ Q_{cU} \emat} \label{moocho:eqn:Qc}.
\end{eqnarray}
%
A valid basis selection can always be determined by simply including all of
the slacks $\breve{s}$ in the full basis and then finding a sub-basis for
$\nabla \breve{c}$.  To show how this can be done, suppose that $\nabla
\breve{c}$ is full column rank and the permutation matrix $(\breve{Q}^x)^T =
{\bmat{cc} (\breve{Q}_{xD})^T & (\breve{Q}_{xI})^T \emat}$ selects a basis
$\breve{C} = (\nabla \breve{c})^T (\breve{Q}_{xD})^T$.  Then the basis
selection for the transformed NLP (with $Q_c = I$) given by
%
\begin{eqnarray}
Q_x & = & {\bmat{ccc}
 \breve{Q}_{xD}  &                &     \\
                 &                & I   \\
                 & \breve{Q}_{xI} &
\emat} \\
C & = & {\bmat{cc} ( \breve{Q}_{xD} \nabla \breve{c} )^T \\ ( \breve{Q}_{xD} \nabla \breve{h} )^T & -I \emat}
	\label{moocho:eqn:C_with_slacks} \\
N & = & {\bmat{c} ( \breve{Q}_{xI} \nabla \breve{c} )^T \\ ( \breve{Q}_{xI} \nabla \breve{h} )^T \emat}
\end{eqnarray}
%
could always be used regardless of the properties or implementation of $\nabla
\breve{h}$.

Notice that basis matrix in (\ref{moocho:eqn:C_with_slacks}) is lower block
triangular with non-singular blocks on the diagonal.  It is therefore straightforward to solve
for linear systems with this basis matrix.  In fact, the direct sensitivity matrix
$D = C^{-1} N$ takes the form
%
\begin{equation}
D = - {\bmat{c}
	( \breve{Q}_{xD} \nabla \breve{c} )^{-T} ( \breve{Q}_{xI} \nabla \breve{c} )^{T} \\
    ( \breve{Q}_{xD} \nabla \breve{h} )^T ( \breve{Q}_{xD} \nabla \breve{c} )^{-T} ( \breve{Q}_{xI} \nabla \breve{c} )^{T}
      -( \breve{Q}_{xI} \nabla \breve{h} )^T
\emat}.
\label{moocho:eqn:D_with_slacks}
\end{equation}
%
Note that if the forward sensitivities $( \breve{Q}_{xD} {}\nabla {}\breve{c}
)^{-T} ( \breve{Q}_{xI} {}\nabla {}\breve{c} )^{T}$ are computed up front then
there is little extra cost in forming this decomposition after the addition of
general inequality constraints.  The structure of
(\ref{moocho:eqn:D_with_slacks}) is significant in the context of active-set
QP solvers that solve the reduced QP subproblem in
(\ref{moocho:eqn:moocho:obj})--(\ref{moocho:eqn:moocho:inequ}) using a
variable-reduction null-space decomposition.  When an implicit adjoint method
is used, a row of $D$ corresponding to a general inequality constraint only
has to be computed if the slack for the constraint is at a bound.  Also note
that the above transformation does not increase the total number of degrees of
freedom of the NLP since $n-m = {}\breve{n}-\breve{m}$.  All of this means
that adding general inequalities to a NLP imparts little extra cost for an
active-set rSQP algorithm if the forward/direct sensitivity method is used or
if these constraints are not active when using the adjoint method.

For reasons of stability and algorithm efficiency, it may be desirable to keep
at least some of the slack variables out of the basis and this can be
accommodated also but is more complex to describe.  For general NLPs solved in
serial, MOOCHO provides support for general inequality constraints and will
automatically add slack variables and perform the needed basis permutations
and partitioning.

Most of the steps in an SQP algorithm do not need to know that there are
general inequalities in the underlying NLP formulation but some steps may,
such as globalization methods and basis selection computations.  Therefore,
those steps in an SQP algorithm that need access to this information are
allowed more detailed access of the underlying NLP in a limited manner.

Now that the basic mathematical context for MOOCHO is in place, we move on to
a description of the basic software infrastructure for MOOCHO.

%
\section{Basic Software Architecture of MOOCHO}
\label{moocho:sec:basic_software_design}
%

MOOCHO is implemented in C++ using advanced object-oriented software
engineering principles.  However, using MOOCHO to solve certain types of NLPs
does not require any deep knowledge of object-orientation or C++.  By copying
and modifying example programs it should be possible for a non-C++ expert to
implement and solve many different NLPs using MOOCHO.  However, solving more
advanced NLPs which utilize specialized application-specific data structures
and linear solvers does require more detailed knowledge of C++ and some
knowledge of object orientation.  Although, the included example applications
should provide a straightforward road map for getting started with such an
application.  For simulation-constrained optimization based on parallel
Epetra\footnote{Epetra is a Trilinos package for distributed-memory vectors
and matrices.}-compatible data structures, using MOOCHO requires almost know
deep knowledge of MOOCHO's interfaces at all.

%
\subsection{High-Level Object Diagram for MOOCHO}
%

There are many different ways to present MOOCHO.  Here, we take a top down
approach.

{\bsinglespace
\begin{figure}[t]
\begin{center}
%\fbox{
\includegraphics*[bb= 0.0in 0.0in 5.2in 4.4in,scale=0.70
]{MoochoObjDiagram}
%}
\end{center}
\caption{
\label{moocho:fig:moocho_obj_diag}
UML object diagram : Course grained object diagram for MOOCHO
}
\end{figure}
\esinglespace}

Figure {}\ref{moocho:fig:moocho_obj_diag} shows a high-level object diagram of
a MOOCHO application, ready to solve a user-defined NLP.  The NLP object
{}\texttt{aNLP} is created by the user and defines the functions and gradients
for the NLP to be solved.  Closely associated with the NLP object is a
{}\texttt{\textit{BasisSystem}} object.  The {}\texttt{\textit{BasisSystem}}
object is used to specify and specialize the implementation of the basis
matrix $C$.  This {}\texttt{\textit{BasisSystem}} object is used by
variable-reduction null-space decompositions.  Each NLP object is expected to
supply a {}\texttt{\textit{BasisSystem}} object.  The NLP and
{}\texttt{\textit{BasisSystem}} objects collaborate with the optimization
algorithm though a set of abstract linear-algebra interfaces.  By creating a
specialized NLP subclass (and the associated linear algebra and
{}\texttt{\textit{BasisSystem}} subclasses) an advanced user can fully take
over the implementation of all of the major linear-algebra computations in a
MOOCHO algorithm.  This includes having full freedom to choose the data
structures for all of the vectors and the matrices $A$, $C$, $N$, $W$ and how
nearly every linear-algebra operation is performed.  This also includes the
ability to use fully transparent parallel linear algebra on a parallel
computer even though none of the core MOOCHO code has any concept of
parallelism.  The linear algebra objects associated with the
{}\texttt{\texttt{NLP}} and {}\texttt{\textit{Basis\-System}} objects define
the foundation for every major computation in a simulation-constrained
optimization (SCOPT) algorithm.  The exact requirements of the application and
the details of the NLP and linear algebra interfaces that satisfy these
requirements are discussed in Section {}\ref{moocho:sec:nlp_and_lin_alg_itfc}.
A complete infrastructure for parallel simulation constrained optimization is
supported through the MOOCHO/Thyra adapters described in the MOOCHO Reference
Manual {}\cite{ref:moochorefguide}.  Directly interacting with the MOOCHO
linear algebra and NLP interfaces is not recommended.  Instead, users are
strongly encouraged to define there SCOPT NLPs through Thyra.

Once a user has developed {}\texttt{\textit{NLP}} and
{}\texttt{\textit{BasisSystem}} classes (i.e.\ indirectly using
{}\texttt{Thyra} and {}\texttt{Thyra::ModelEvaluator}) for their specialized
application, an {}\texttt{\textit{NLP}} object can be passed on to a
{}\texttt{Moocho\-Solver} object.  The {}\texttt{Moocho\-Solver} class is a
convenient ``facade'' (see {}\cite{ref:gama_et_al_1995}) that brings together
many different components that are needed to build a complete optimization
algorithm in a way that is transparent to the user.  The
{}\texttt{Moocho\-Solver} object will instantiate an optimization algorithm
(given a default or a user-defined configuration object) and will then solve
the NLP, returning the solution (or partial solution on failure) to the
{}\texttt{\textit{NLP}} object itself.  Figure
{}\ref{moocho:fig:moocho_obj_diag} also shows the course grained layout of a
MOOCHO algorithm.  An advanced user can solve even the most complex
specialized NLP without needing to understanding how these algorithmic objects
work together to implement an optimization algorithm.  One only needs to
understand the algorithmic framework in order to tinker with the optimization
algorithms themselves.  Understanding the underlying algorithmic framework is
crucial for algorithm developers through.

While MOOCHO offers complete flexibility to solve many different types of
specialized NLPs in diverse application areas such as dynamic optimization and
control (see {}\cite{ref:biegler_et_al_2001}) and PDEs (see
{}\cite{ref:biros_1999}) it can also be used to solve more generic NLPs such
as are supported by modeling systems like GAMS {}\cite{ref:brooke_gams_1997}
or AMPL {}\cite{ref:ampl_1993}.  For serial NLPs which can compute explicit
Jacobian entries for $A$, all that a user needs to do is to create a subclass
of {}\texttt{NLPSerialPreprocessExplJac} and define the problem functions and
derivatives.  For these types of NLPs, a default
{}\texttt{\textit{BasisSystem}} subclass is already defined which can use one
of a number of different dense or sparse direct linear solvers to implement
all of the required functionality.  A simple example NLP that derives from
{}\texttt{NLPSerialPreprocessExplJac} is described in the MOOCHO Reference
Manual {}\cite{ref:moochorefguide}.

%
\section{Overview of NLP and Linear-Algebra Interfaces}
\label{moocho:sec:nlp_and_lin_alg_itfc}
%

All of the high-level optimization code in MOOCHO is designed to allow
arbitrary implementations of the linear-algebra objects.  It is the NLP object
that defines the basic foundation for all of the linear algebra used by a SCOPT
optimization algorithm.  The NLP object accomplishes this by exposing a set of
abstract linear algebra objects.  Before the specifics of the NLP and linear
algebra interfaces are described, the specific requirements for SCOPT
optimization algorithms are described in Section
{}\ref{moocho:sec:nlp_requirements}.  This is followed by the descriptions of
the linear algebra and NLP interfaces in Sections
{}\ref{moocho:sec:ALAP_overview} and
{}\ref{moocho:sec:nlpinterfacepack_overview} respectively.

%
\subsection{Basic Application Requirements for Simulation-Constrained Optimization with MOOCHO}
\label{moocho:sec:nlp_requirements}
% 

The requirements for large-scale gradient-based SCOPT optimization
algorithms implemented in MOOCHO can broken down into three different
levels: \textit{direct SCOPT}, \textit{adjoint SCOPT}, and
{}\textit{full-Newton SCOPT}.  These three levels represent different
levels of intrusiveness and functionality from the underlying
application that are used the implement the NLP.

{\bsinglespace
\begin{table}
{\small\begin{center}
\begin{tabular}{|c|c|l|}
\hline
%
\begin{minipage}{12ex}
\begin{center}
\hspace{0in}\\
Optimization level
\end{center}
\end{minipage}
%
& \begin{minipage}{70ex}
\begin{center}
\hspace{0in}\\
Application requirements \\
(additive between levels) \\
\end{center}
\end{minipage} \\
%
\hline
{}\textit{Direct SCOPT}
& \begin{minipage}{70ex}
\hspace{0in}\\
\begin{tabular}{ll}
Evaluation of objective: & $x \in \mathcal{X} \rightarrow f \in \RE$ \\
Evaluation of constraints residual: & $x  \in \mathcal{X} \rightarrow c \in \mathcal{C}$ \\
Evaluation of objective gradient: & $x  \in \mathcal{X} \rightarrow \nabla f \in \mathcal{X}$ \\
Evaluation of direct sensitivity matrix: & $D = -C^{-1} N  \in \mathcal{X}_D|\mathcal{X}_I$ \\
Evaluation of Newton step: & $p_y = -C^{-1} c(x)  \in \mathcal{X}_D$ \\
\end{tabular}
\end{minipage} \\
\hline
{}\textit{Adjoint SCOPT}
& \begin{minipage}{70ex}
\hspace{0in}\\
Ability to perform mat-vec products: \\
$p = A q, \; q = A^{T} q, \; \mbox{for} \; q \in \mathcal{C}, p \in \mathcal{X}$ \\
Ability to solve linear systems: \\
$p = C^{-1} q, \; q = C^{-T} q, \; \mbox{for} \; q \in \mathcal{C}_d, p \in \mathcal{X}_D$ \\
\end{minipage} \\
\hline
{}\textit{Full-Newton SCOPT}
& \begin{minipage}{70ex}
\hspace{0in}\\
Ability to perform mat-vec products:\\
$p = W q, \; \mbox{for} \; q \in \mathcal{X}, p \in \mathcal{X}$ \\
\end{minipage} \\
\hline
\end{tabular}
\end{center}}
\caption[Minimum Application Requirements for levels of invasiveness
 for Simulation-Constrained Optimization (SCOPT)]{
\label{moocho:tbl:SCOPT_requirements}
Minimum Application Requirements for levels of invasiveness for
Simulation-Constrained Optimization (SCOPT).  }
\end{table}
\esinglespace}

The most basic level of requirements are for \textit{direct SCOPT} methods.
This level only requires forward linear solves with the basis matrix for
specific right-hand-side vectors.  Most applications that utilize an exact
Newton-type method for solving the simulation problem can compute the
solutions to these linear systems.  Both the orthogonal and the coordinate
variable-reduction null-space decompositions can be implemented with just the
quantities $D = -C^{-1} N$ and $p_y = C^{-1} c$.  In addition, many different
types of globalization methods can also be used (both line search and trust
region methods complete with second-order corrections for the constraints).

The next level of requirements is for \textit{adjoint SCOPT} methods.  This
level requires the ability to perform mat-vec products and linear solves with
the non-transposed and transposed basis $C$ and non-basis $N$ matrices with
arbitrary vectors.  More efficient and robust optimization algorithms can be
implemented using this functionality.  For example, the ability to solve for
transposed systems with the basis matrix $C$ provides the ability to compute
estimates for the Lagrange multipliers $\lambda$ and the ability to compute
reduced gradients at a cost independent of the number of optimization
parameters.

The highest level of requirements is for \textit{full-Newton SCOPT} methods.
The minimum requirements for these methods is the ability to compute mat-vec
products with an approximation of the Hessian of the Lagrangian $W$.  MOOCHO
currently does not implement a full-Newton (i.e.\ full-space) SQP method.

The last set of requirements for SCOPT methods is the requirements on
vectors.  There is a great diversity of specialized vector or array
operations that optimization methods must perform.  This difficult set
of requirements is handled by a design for vector
reduction/transformation operators (RTOp) which is described in
{}\cite{ref:rtop_toms} and mentioned in Section
{}\ref{moocho:sec:ALAP_overview}.

Note that this set of requirements satisfies all the requirements of the SCOPT
optimization interfaces described in {}\cite{ref:opt_ctrl_itfc}.

%
\subsection{Overview of {}\texttt{AbstractLinAlgPack}: Interfaces to Linear Algebra Objects}
\label{moocho:sec:ALAP_overview}
%

The linear algebra interfaces described in this section serve two roles.  The
first role is to abstract the linear algebra objects associated with the NLP
interface such as the vector objects for the unknowns $x$, the residual of the
constraints $c$ and the gradient of the objective function $\nabla f$; and the
matrix objects for the gradients of the constraints $A$, the Hessian of the
Lagrangian $W$ and the variable-reduction matrices $C$, $N$, $E$ and $F$.  The
second role of the linear algebra interfaces is to abstract objects that are
specific to the optimization algorithms such as for quasi-Newton
approximations for the Hessian and the reduced Hessian of the Lagrangian.  The
objects from the latter group are obviously dependent on objects from the
former group in various ways.  This latter role greatly increases the
complexity and functionality of these interfaces.

Figure {}\ref{moocho:fig:AbstractLinAlgPack} shows a UML class diagram of the
basic linear algebra abstractions.  The foundation for all the linear algebra
is a vector space.  A vector space object is represented though an abstract
interface called {}\texttt{\textit{VectorSpace}}.  A
{}\texttt{\textit{VectorSpace}} object primarily acts as an ``abstract
factory'' {}\cite{ref:gama_et_al_1995} and creates vectors from the vector
space using the {}\texttt{\textit{create\_member()}} method.
{}\texttt{\textit{Vector\-Space}} objects can also be used to check for
compatibility using the {}\texttt{\textit{is\_compatible()}} method.  Every
{}\texttt{\textit{Vector\-Space}} object has a dimension.  Therefore a
{}\texttt{\textit{Vector\-Space}} object can not be used to represent an
infinite-dimensional vector space.  Every vector space object is also equipped
with an scalar (i.e.\ inner) product that is used to introduce scaling into
the problem as described in {}\cite{ref:opt_ctrl_itfc}.  Just because two
vectors from different vector spaces have the same dimension (and the same
inner product) does not automatically imply that the implementations will be
compatible.  For example, distributed parallel vectors may have the same
global dimension but the vector elements may be distributed to processors
differently (we say that they have different ``maps'') and are therefore not
easily compatible in the RTOp sense.  This is an important concept to
remember.

{\bsinglespace
\begin{figure}[t]
\begin{center}
%\fbox{
\includegraphics*[bb= 0.0in 0.0in 7.9in 5.6in,scale=0.70
]{AbstractLinAlgPack}
%}
\end{center}
\caption{\label{moocho:fig:AbstractLinAlgPack}
UML class diagram : {}\texttt{AbstractLinAlgPack}, abstract interfaces to linear algebra
}
\end{figure}
\esinglespace}

Vector implementations are abstracted behind interfaces.  The vector interface
is broken up into two levels: {}\texttt{\textit{Vector}} and
{}\texttt{\textit{Vector\-Mutable}}.  The {}\texttt{\textit{Vector}} interface
is an immutable interface where vector objects can not be changed by the
client.  The {}\texttt{\textit{Vector\-Mutable}} interface extends the
{}\texttt{\textit{Vector}} interface in allowing clients to change the
elements in the vector.  These vector interfaces are very powerful and allow
the client to perform many different types of operations.  The foundation of
all vector functionality is the ability to allow clients to apply user-defined
{}\texttt{RTOp} operators to perform arbitrary reductions and transformations
(see the method {}\texttt{\textit{apply\_op(...)}}). The ability to write
these types of user-defined operators is critical to the implementation of
advanced optimization algorithms {}\cite{ref:rtop_toms}.  A single RTOp
application method is the only method that a vector implementation is required
to provide (in addition to some trivial methods such as returning the vector
space object) which makes it fairly easy to add a new vector implementation.
In addition to allowing clients to apply {}\texttt{RTOp} operators, the other
major feature is the ability to create arbitrary subviews of a vector (using
the {}\texttt{\textit{sub\_view()}} methods) as abstract vector objects.  This
is an important feature in that it allows the optimization algorithm to access
the subvectors associated with the dependent (i.e.\ state) and independent
(i.e.\ design) variables separately (in addition to any other arbitrary range
of vector elements).  Support for subviews is provided by default by every
vector implementation through default view subclasses (see the class
{}\texttt{Vector\-Mutable\-Subview}) that rely only on the {}\texttt{RTOp}
application methods.  The last bit of major functionality is the ability of
the client to extract an explicit view of a subset of the vector elements.
This is needed in a few parts of an optimization algorithm for such tasks as
dense quasi-Newton updating of the reduced Hessian in a serial setting and the
implementation of the compact LBFGS matrix in a distributed parallel setting.
Aside from vectors being important in their own right, vectors are also the
major type of data that is communicated between higher-level interfaces such
as linear operators (i.e.\ matrices) and function evaluators (i.e.\ the NLP
interface).

The basic matrix (i.e.\ linear operator) interfaces are also shown in Figure
{}\ref{moocho:fig:AbstractLinAlgPack}.  The {}\texttt{\textit{Matrix\-Op}}
interface is for general rectangular matrices.  Associated with any
{}\texttt{\textit{Matrix\-Op}} object is a column space and a row space shown
as {}\texttt{space\_cols} and {}\texttt{space\_rows} respectively in the
figure.  Since column and row {}\texttt{\textit{Vector\-Space}} objects have a
finite dimension, this implies that every matrix object also has finite row
and column dimensions.  Therefore, these matrix interfaces can not be used to
represent infinite-dimensional linear operators.  The column and row spaces of
a matrix object identify the vector spaces for vectors that are compatible
with the columns and rows of the matrix respectively.  For example, if the
matrix $A$ is represented as a {}\texttt{\textit{Matrix\-Op}} object then the
vectors $y$ and $x$ would have to lie in the column and row spaces
respectively in order to perform the matrix-vector product $y = A x$.  Note
that despite name, a {}\texttt{\textit{Matrix\-Op}} object does not provide
any type of efficient access to matrix elements.  If explicit matrix elements
are required, then the matrix object can support other defined matrix
interfaces in order to extract the elements in a sparse (see the interfaces
{}\texttt{\textit{Matrix\-Extract\-Sparse\-Elements}} and
{}\texttt{\textit{Matrix\-Convert\-To\-Sparse}}) or dense (see the interfaces
{}\texttt{\textit{Matrix\-Op\-Get\-GMS...}}) format.

These matrix interfaces go beyond what most other abstract
matrix/linear-operator interfaces have attempted.  Other abstract
linear-operator interfaces only allow the forward applications of $y = A x$ or
the transpose (adjoint) $y = A^T x$ for vector-vector mappings.  In addition
to this basic functionality, every {}\texttt{\textit{Matrix\-Op}} object can
provide arbitrary subviews as {}\texttt{\textit{Matrix\-Op}} objects through
the {}\texttt{\textit{sub\_view(...)}} methods.  These methods have default
implementations based on default view subclasses which, fundamentally, is
supported by the ability to take arbitrary subview of vectors.  This ability
to create these subviews is critical in order to access the basis matrices in
(\ref{moocho:eqn:basis_partitioning}) given a Jacobian object {}\texttt{Gc}
for $\nabla c$.  These matrix interfaces also allow much more general types of
linear-algebra operations.  The {}\texttt{\textit{Matrix\-Op}} interface
allows the client to perform level 1, 2 and 3 BLAS operations (see [???] for a
discussion of the convention for naming functions for linear-algebra
operations)

{\bsinglespace
\begin{eqnarray*}
B & = & \alpha \, op(A) + B \\
y & = & \alpha \, op(A) \,  x + \beta y \\
C & = & \alpha \, op(A) \, op(B) + \beta C.
\end{eqnarray*}
\esinglespace}

One of the significant aspects of these linear-algebra operations is
that an abstract {}\texttt{\textit{Matrix\-Op}} object can appear on
the left-hand-side.  This adds a whole set of issues (i.e.\ multiple
dispatch) that are not present in other linear-algebra interfaces.

The matrix interfaces assume that the matrix operator or the transpose
of the matrix operator can be applied.  Therefore, a correct
{}\texttt{\textit{Matrix\-Op}} implementation must be able to perform
the transposed as well as the non-transposed operation.  This
requirement is important when the NLP interfaces are discussed later.

Of all of the functionality in the {}\texttt{\textit{Matrix\-Op}} interface,
the only pure virtual method is the method for the level-2 BLAS operation for
matrix-vector multiplication.  All other methods have reasonable default
implementations based on this one method.  Therefore, generating a new
concrete {}\texttt{\textit{Matrix\-Op}} subclass is usually fairly easy.  If
the default implementations of some of the other methods are found to be
inefficient in important cases, then they can be overridden to provide better,
more specialized implementations.  This design allows for a pay-as-you-go
approach to developing implementations of linear algebra objects and this
philosophy applies to all of the linear algebra interfaces in
{}\texttt{Abstract\-Lin\-Alg\-Pack} as well.

Several specializations of the {}\texttt{\textit{Matrix\-Op}} interface are
also required in order to implement an advanced optimization algorithm.  All
symmetric matrices are abstracted by the {}\texttt{\textit{Matrix\-Sym\-Op}}
interface.  This interface is required in order for the operation $C =
\alpha\,op(B)\,op(A)\,op(B^T) + \beta C$ to be guaranteed to maintain the
symmetry of the matrix $C$.  Note that a symmetric matrix requires that the
column and row spaces be the same.

The specialization {}\texttt{\textit{Matrix\-Op\-Nonsing}} is
for nonsingular square matrices that can be used to solve for linear
systems.  As a result, the level-2 and level-3 BLAS operations

{\bsinglespace
\begin{eqnarray*}
y & = & op(A^{-1}) \, x \\
C & = & \alpha \, op(A^{-1}) \, op(B) \\
C & = & \alpha \, op(B) \, op(A^{-1})
\end{eqnarray*}
\esinglespace}

\noindent{}are supported.  The solution of linear systems represented by these
operations can be implemented in a number of different ways.  A direct
factorization followed by back solves or alternatively a preconditioned
iterative solver (i.e.\ GMRES or some other Krylov subspace method) could be
used.  Or, a more specialized solution process could be employed which is
tailored to the special properties of the matrix (i.e.\ banded matrices).

The last major matrix interface
{}\texttt{\textit{Matrix\-Sym\-Op\-Nonsing}} is for symmetric
nonsingular matrices.  This interface allows the implementation of the
operation $C = \alpha\,op(B)\,op(A^{-1})\,op(B^T)$ and guarantees
that $C$ will be a symmetric matrix.

Figure {}\ref{moocho:fig:AbstractLinAlgPack} shows two other specializations
of the {}\texttt{Matrix\-Op} interface that have not been discussed yet,
{}\texttt{Multi\-Vector} and {}\texttt{Multi\-Vector\-Mutable}.  A
multi-vector is special kind of matrix where access the rows, columns and/or
diagonals may be permitted as {}\texttt{\textit{Vector}} and
{}\texttt{\textit{Vector\-Mutable}} views.  The primary role for a
multi-vector object is the creation of tall, thin matrices where each column
vector is accessible.  It is these types of
{}\texttt{\textit{Vector\-Mutable}} objects that are created by the
{}\texttt{\textit{create\_members(num\_vecs)}} method on
{}\texttt{\textit{Vector\-Space}}.  The row space for these types of
{}\texttt{\textit{Multi\-Vector\-Mutable}} {}\texttt{\textit{Matrix\-Op}}
objects are assumed to be small, serial vector spaces in all cases.  The
ability of a {}\texttt{\textit{Vector\-Space}} object to create
{}\texttt{\textit{Multi\-Vector\-Mutable}} objects with an arbitrary number of
columns implies that every {}\texttt{\textit{Vector\-Space}} object can create
other small serial {}\texttt{\textit{Vector\-Space}} objects of arbitrary
dimension.  In order to directly allow this functionality, the method
{}\texttt{\textit{small\_vec\_spc\_fcty()}} (not shown) returns a factory
object for creating these vector spaces.  Since
{}\texttt{\textit{Multi\-Vector}} is a type of {}\texttt{\textit{Matrix\-Op}},
it can be passed into all of the level-3 BLAS methods on
{}\texttt{\textit{Matrix\-Op}} and {}\texttt{\textit{Matrix\-Op\-Nonsing}}.
By passing a {}\texttt{\textit{Multi\-Vector\-Mutable}} object (from the
correct vector space) as the target object for any of these linear algebra
operations guarantees that the operation will be supported since it can always
be performed, column by column, using the level-2 BLAS methods.

\label{moocho:page:BasisSystem}

A major part of an rSQP algorithm, based on a variable-reduction null-space
decomposition, is the selection of a basis.  The fundamental abstraction for
this task is the {}\texttt{\textit{Basis\-System}} interface (as first
introduced in Figure {}\ref{moocho:fig:moocho_obj_diag}).  The
{}\texttt{\textit{update\_basis()}} method takes the rectangular Jacobian
{}\texttt{Gc} ($\nabla c$) and returns a
{}\texttt{\textit{Matrix\-Op\-Nonsing}} object for the basis matrix $C$.  This
interface assumes that the variables are already sorted according to
(\ref{moocho:eqn:x_D_I}).  For many applications, the selection of the basis
is known {\em a priori} (e.g.\ simulation-constrained optimization).  For
other applications, it is not clear what the best basis selection should be.
For the latter type of application, the basis selection can be performed
on-the-fly and result in one or more different basis selections during the
course of an optimization algorithm.  The
{}\texttt{\textit{Basis\-System\-Perm}} specialization allows the optimization
algorithm to either ask the basis system object for a good basis selection
(\texttt{\textit{select\_basis()}}) or can tell the basis system object what
basis to use (\texttt{\textit{set\_basis()}}).  The selection of dependent
$x_D$ and independent $x_I$ variables and the selection of the decomposed
$c_d(x)$ and undecomposed $c_u(x)$ constraints is represented by
{}\texttt{\textit{Permutation}} objects.  The protocol for handling basis
changes is somewhat complicated and is beyond the scope of this discussion.
Note that the {}\texttt{\textit{Basis\-System\-Perm}} interface is optional
and does not have to be supported by an application.

It is likely that a future version of MOOCHO might use a set of linear algebra
interfaces that is directly based on the new Thyra interfaces that are part of
Trilinos.  If this happens, many of these interfaces will be phased out and/or
transitioned to Thyra.

%
\subsection{Overview of {}\texttt{NLPInterfacePack}: Interfaces to Nonlinear Programs}
\label{moocho:sec:nlpinterfacepack_overview}
%

The hierarchy of NLP interfaces that all MOOCHO optimization
algorithms are based on is shown in Figure
{}\ref{moocho:fig:NLPInterfacePack}.  These NLP interfaces act primarily
as evaluators for the functions and gradients that define the NLP.
These interfaces represent the various levels of intrusiveness into an
application model.

The base-level NLP interface is called {}\texttt{\textit{NLP}} which defines
the nonlinear program.  An {}\texttt{\textit{NLP}} object defines the vector
spaces for the variables $\mathcal{X}$ and the constraints $\mathcal{C}$ as
{}\texttt{\textit{Vector\-Space}} objects {}\texttt{space\_x} and
{}\texttt{space\_c} respectively.  The {}\texttt{\textit{NLP}} interface
allows access to the initial guess of the solution $x_0$ and the bounds $x_L$
and $x_U$ as {}\texttt{\textit{Vector}} objects {}\texttt{x\_init},
{}\texttt{xl} and {}\texttt{xu} respectively.  This interface also provides
access to {}\texttt{\textit{Permutation}} objects {}\texttt{P\_var} and
{}\texttt{P\_var} for permutation matrices $Q_x$ and $Q_c$, respectively.
These matrices are used to permute from the original order of variables and
constraints to the current basis selection (see Section
{}\ref{moocho:sec:nlp_with_slacks}).

The {}\texttt{\textit{NLP}} interface allows clients to evaluate just the
zero-order quantities $f(x) \in \RE$ and $c(x) \in \mathcal{C}$ as scalar and
{}\texttt{\textit{Vector\-Mutable}} objects respectively.  This is useful
since many different steps in an optimization algorithm do not require
derivatives for the problem functions.  Examples include several different
line search and trust region globalization methods (i.e.\ Filter and exact
merit function).  Non-gradient-based optimization methods could also be
implemented through this interface but smoothness and continuity of the
variables and functions is assumed by default.  Note that this interface is
the same as a NAND (nested analysis and design) approach if there are no
equality constraints (i.e.\ removed using nonlinear elimination).  The
{}\texttt{\textit{NLP}} interface can also be used for unconstrained
optimization (i.e.\ $|\mathcal{C}| = m = 0$) or for a system of nonlinear
equations (i.e.\ $|\mathcal{X}| = n = |\mathcal{C}| = m$).

As mentioned in Section {}\ref{moocho:sec:nlp_with_slacks}, some parts of an
optimization algorithm can benefit greatly from knowing about general
inequality constraints and become less effective when these constraints are
converted to equalities using slack variables.  These steps in the
optimization algorithm can compute the quantities $\breve{c}(\breve{x})$ and
$\breve{h}(\breve{x})$ independently and access the bounds $\breve{h}_L$ and
$\breve{h}_U$ through the {}\texttt{\textit{NLP}} interface as well.

{\bsinglespace
\begin{figure}[t]
\begin{center}
%\fbox{
\includegraphics*[bb= 0.0in 0.0in 4.2in 4.05in,scale=0.80
]{NLPInterfacePack}
%}
\end{center}
\caption{
\label{moocho:fig:NLPInterfacePack}
UML class diagram : {}\texttt{NLPInterfacePack}, abstract interfaces nonlinear programs
}
\end{figure}
\esinglespace}

The next level of NLP interface is {}\texttt{\textit{NLPObjGrad}}.  This
interface simply adds the ability to compute the gradient of the objective
function $\nabla f(x) \in \mathcal{X}$ as a {}\texttt{\textit{VectorMutable}}
object {}\texttt{Gf}.  For many applications, it is far easier and less
expensive to compute derivatives for the objective function than it is for the
constraints.  That is why this functionality is considered more general than
sensitivities for the constraints\footnote{It turns out that the assumption
that getting derivatives of the objective function is easier than getting
derivatives for the constraints is not true in general, especially for
simulation-constrained optimization problems.  Therefore, these interfaces
will likely be reworked at some point in the future to better reflect
reality.}  and is therefore higher in the inheritance hierarchy than
interfaces the include derivatives for $\nabla c$.

Derivatives for the constraints $\nabla c$ are broken up into two separate
interfaces based on the requirements for \textit{direct SCOPT} verses
{}\textit{adjoint SCOPT} methods.  These interfaces represent the different
capabilities of the underlying application code.

For applications that can only satisfy the requirements for {}\textit{direct
SCOPT} there is the {}\texttt{\textit{NLP\-Direct}} interface.  As the name
implies, the {}\texttt{\textit{NLP\-Direct}} interface only requires the
direct sensitivity matrix $D = -C^{-1} N$ and the solution to the Newton
linear systems $p_y = C^{-1} c$.  With usually minor modifications, almost any
application code that uses a Newton method for the forward solution can be
used to implement the {}\texttt{\textit{NLPDirect}} interface.

The {}\texttt{\textit{NLP\-First\-Order}} interface is for applications can
implement the requirements for \textit{adjoint SCOPT} methods.  This NLP
interface assumes that the application can, at the very least, form and
maintain a {}\texttt{\textit{Matrix\-Op}} object {}\texttt{Gc} for the
gradient of the constraints $\nabla c$.  Recall that this implies that mat-vec
products with both $\nabla c^T$ \underline{and} $\nabla c$.  Note that
operations of the form $p = \nabla c^T q$ can always be approximated using
directional finite differences (i.e.\ $p = \nabla c^T \, q \approx
\lim_{\epsilon \rightarrow 0} ( c(x+\epsilon q) - c(x) ) / \epsilon )$) but
operations of the form $q = \nabla c \, p$ can not.  Therefore, this interface
can not simply be approximated using finite differences.  However, the reverse
mode of AD can generally be used to implement products of the form $q = \nabla
c \, p$ in an efficient manner without having to actually form the matrix
object $\nabla c$ first {}\cite{ref:adolc_1996}.  In order to fully support
the requirements for {}\textit{adjoint SCOPT} methods, a
{}\texttt{\textit{NLP\-First\-Order}} object must supply a
{}\texttt{\textit{Basis\-System}} object that may be specialized for the
application's {}\texttt{Gc} matrix object.

The highest level of requirements for \textit{full-Newton SCOPT} methods is
satisfied by the {}\texttt{\textit{NLP\-Second\-Order}} interface.  This NLP
interface allows the optimization algorithm to compute a
{}\texttt{\textit{Matrix\-Sym\-Op}} matrix object {}\texttt{HL} for the
Hessian of the Lagrangian $W = \nabla^2_{xx} L = \nabla^2 f(x) + \sum^m_{j=1}
\lambda_j \nabla^2 c_j(x)$.  How this Hessian matrix object is used can vary
greatly.  This matrix object can be used to compute the exact reduced Hessian
$B = Z^T W Z$ or can be used to form the full KKT matrix (in some cases).
Many other possibilities exist but the best approach will be very much
application dependent.  MOOCHO currently does not support a full-space SQP
algorithm and therefore there are currently no facilities for solving linear
system with the KKT matrix $W$.  However, in the past, this interface has been
used within MOOCHO to compute the exact reduced Hessian and use it instead of
a quasi-Newton approximation.

Figure {}\ref{moocho:fig:NLPInterfacePack} shows another NLP interface that
has not been discussed yet, {}\texttt{\textit{NLP\-Var\-Reduct\-Perm}}.  This
interface allows the NLP object and MOOCHO optimization algorithm to
collaborate in changing the basis and defining new permutations for the
variables and the constraints shown as the permutation matrices
{}\texttt{P\_var} and {}\texttt{P\_var} on the {}\texttt{\textit{NLP}}
interface respectively.  This interface is considered a mix-in interface that
concrete NLP subclasses should only support if changing the basis selection is
possible.  If an NLP subclass supports the
{}\texttt{\textit{NLP\-Var\-Reduct\-Perm}} and
{}\texttt{\textit{NLP\-First\-Order}} interfaces, it is also required that the
{}\texttt{\textit{Basis\-System}} object exposed by the
{}\texttt{\textit{NLP\-First\-Order}} interface also support the
{}\texttt{\textit{Basis\-System\-Perm}} interface.  A carefully designed
collaboration between the {}\texttt{\textit{NLP\-Var\-Reduct\-Perm}} and
{}\texttt{\textit{Basis\-System\-Perm}} interfaces, which is mediated by a
MOOCHO algorithm, makes it possible for the basis selection to change during
the course of an algorithm.  This functionality will generally only be
supportable by NLPs that provide explicit Jacobian entries and use direct
linear solvers.  For most specialized applications, the selection of the basis
is fixed and unchangeable (e.g.\ simulation-constrained optimization).
Therefore, an NLP subclass does not have to support the
{}\texttt{\textit{NLP\-Var\-Reduct\-Perm}} or
{}\texttt{\textit{Basis\-System\-Perm}} interfaces to be used with MOOCHO.

In summary, the {}\texttt{\textit{NLP}}, {}\texttt{\textit{NLP\-Direct}},
{}\texttt{\textit{NLP\-First\-Order}} and
{}\texttt{\textit{NLP\-Second\-Order}} interfaces represent the four different
levels of invasiveness to applications for optimization.  The
{}\texttt{\textit{NLP}} interface without equality constraints can used to
implement basic NAND (i.e.\ black-box) optimization algorithms while on the
other extreme the {}\texttt{\textit{NLP\-Second\-Order}} interface can be used
to implement fully-coupled invasive SCOPT methods with access to second
derivatives\footnote{Again, in theory these interfaces can support full-space
Newton SQP methods but this has not been implemented in MOOCHO at this point}.

%
\section{Defining Optimization Problems}
\label{moocho:sec:defining_opt_problems}
%

Optimization problems for MOOCHO can be defined in primarily two different
ways.  First, a general NLP with explicit first-derivative entries can be
defined by creating a subclass of
{}\texttt{NLPInterfacePack\-::NLP\-Serial\-Preprocess\-Expl\-Jac}.  This type
of NLP can only be solved using a single process (i.e.\ no MPI parallelism)
and a sparse direct linear solver must be used (i.e.\ MA28).  For this type of
NLP, there is not need for the user to partition the variables into dependent
variables (i.e.\ state variables) and independent variables (i.e.\
optimization parameters).

The second type of NLP that can be solved using MOOCHO are
simulation-constrained NLPs where the basis section is known up front.  For
these types of NLPs, it is recommended that the NLP be specified through the
{}\texttt{Thyra\-::Model\-Evaluator} interface (or the
{}\texttt{EpetraExt\-::Model\-Evaluator} interface for Epetra-based
applications) and this provides access to a significant linear solver
capability through Trilinos.  These types of NLPs can be solved in single
program multiple data (SPMD) mode in parallel on a massively parallel
computer.  This is the recommended interface for SCOPT applications to adopt
and this will most likely drive the development of MOOCHO in the near future.

See the MOOCHO Reference Manual {}\cite{ref:moochorefguide} for examples of
these different types of NLPs.

%
\section{Basic properties of MOOCHO Algorithms}
\label{moocho:sec:basic_algo_properties}
%

All MOOCHO algorithms share a few different properties that are described
below.

%
\subsection{Solver options}
\label{moocho:sec:solver_options}
%

Various options can be set in a flexible and user friendly format.  Using this
format, options are clustered into different ``options groups''.  An example
option file containing many off the typical options that a user would set is
shown below:

{\scriptsize\begin{verbatim}
begin_options

options_group NLPSolverClientInterface {
    max_iter = 20;
    max_run_time = 2.0; *** In minutes
    opt_tol = 1e-2;
    feas_tol = 1e-7;
*    journal_output_level = PRINT_NOTHING;
*    journal_output_level = PRINT_BASIC_ALGORITHM_INFO;
    journal_output_level = PRINT_ALGORITHM_STEPS;
*    journal_output_level = PRINT_ACTIVE_SET;
*    journal_output_level = PRINT_VECTORS;
*    journal_output_level = PRINT_ITERATION_QUANTITIES;
*    null_space_journal_output_level = DEFAULT;
*    null_space_journal_output_level = PRINT_ACTIVE_SET;
*    null_space_journal_output_level = PRINT_VECTORS;
    null_space_journal_output_level = PRINT_ITERATION_QUANTITIES;
    journal_print_digits = 10;
*    check_results = true;  *** (costly?)
    check_results = false; *** [default]
    calc_conditioning = true;
    calc_matrix_norms = true;  *** (costly?)
    calc_matrix_info_null_space_only = true;  *** (costly?)
}

options_group DecompositionSystemStateStepBuilderStd {
*    null_space_matrix = AUTO;     *** Let the solver decide [default]
    null_space_matrix = EXPLICIT; *** Compute and store D = -inv(C)*N explicitly
*    null_space_matrix = IMPLICIT; *** Perform operations implicitly with C, N
*    range_space_matrix = AUTO;        *** Let the algorithm decide dynamically [default]
*    range_space_matrix = COORDINATE;  *** Y = [ I; 0 ] (Cheaper computationally)
    range_space_matrix = ORTHOGONAL;  *** Y = [ I; -N'*inv(C') ] (more stable)
}

options_group NLPAlgoConfigMamaJama {
*    quasi_newton = AUTO;   *** Let solver decide dynamically [default]
    quasi_newton = BFGS;   *** Dense BFGS
*    quasi_newton = LBFGS;  *** Limited memory BFGS
*    line_search_method = AUTO;    *** Let the solver decide dynamically [default]
*    line_search_method = NONE;    *** Take full steps at every iteration
*    line_search_method = DIRECT;  *** Use standard Armijo backtracking
    line_search_method = FILTER;  *** [default] Use the Filter line search method
}

end_options

\end{verbatim}}

These and many other options may be included in the {}\texttt{Moocho.opt}
file.  See the MOOCHO Reference Manual {}\cite{ref:moochorefguide} for the
listing of all of the valid options with some documentation.  Note that the
mathematical description itself in Section {}\ref{moocho:sec:sqp_background}
is critical in understanding and interpreting these options.

A skeleton for a {}\texttt{Moocho.opt} file can be created using the
{}\texttt{generate-opt-file.pl} Perl script {}\cite{ref:moochorefguide}.  This
script can be run from the source tree and it also gets installed in
{}\texttt{\$TRILINOS\-\_INSTALL\-\_DIR/tools/moocho}.

Documenting MOOCHO is a major task and this issue is discussed in more detail
in the next section.

%
\subsection{Algorithm Description and Iteration Output}
\label{moocho:sec:algo_descr_iter_out}
%

One of the greatest challenges in developing software of any kind is in
maintaining documentation.  This is especially a problem with software
developed in a research environment.  Without good documentation, software can
be very difficult to understand and maintain.  In addition to the Doxygen
generated documentation, which is very effective in describing interfaces and
other specifications, there is also a need to document the more dynamic parts
of an optimization algorithm.  Highly flexible and dynamic software, which
MOOCHO is designed to be, can be very hard to understand just by looking at
the source code and static documentation.

A problem that often occurs with numerical research codes is that the
algorithm described in some paper is not what is actually implemented in the
software.  This can cause great confusion later on when someone else tries to
maintain the code.  Some of these discrepancies are only minor implementation
issues while others seriously impact the behavior of the algorithm.

Primarily, two features have been implemented to aid in the documentation of a
MOOCHO algorithm: the configured algorithm description can be printed out
before the algorithm is run, and information is output about a running
algorithm.

The first feature is that a printout of a configured MOOCHO algorithm can be
produced by setting the option
{}\texttt{Moocho\-Solver\{\-print\-\_algo\-=true\-\}}, where this is shorthand
for the {}\texttt{print\-\_algo} option in the {}\texttt{Moocho\-Solver}
options group.  With this option set to {}\texttt{true}, the algorithm
description is printed to the {}\texttt{MoochoAlgo.out} file before the
algorithm is run.  The algorithm is printed using Matlab-like syntax.  The
identifier names for iteration quantities used in this printout are largely
the same as used in the source code.  There is a very careful mapping between
the names used in the mathematical notation of the SQP algorithm and the
identifiers used in the source code and algorithm printout.  This mapping for
identifiers is given in Appendix {}\ref{app:moocho_nomenclature_summary}.
Each iteration quantity name in the algorithm printout has {}\texttt{'\_k'},
{}\texttt{'\_kp1'} or {}\texttt{'\_km1'} appended to the end of it to
designate the iterations $(k)$, $(k+1)$ or $(k-1)$ respectively, for which the
quantity was calculated.  Much of the difficulty in understanding an
algorithm, whether in mathematical notation or implemented in source code, is
in knowing precisely what a quantity represents.  By using a careful mapping
of names and identifiers, it is much easier to understand and maintain
numerical software.

This algorithm printout is put together by the {}\texttt{NLP\-Algo} object
(through functionality in the base class
{}\texttt{Iteration\-Pack\-::Algorithm}) as well as the
{}\textit{\texttt{Algorithm\-Step}} objects.  Each step is responsible for
printing out its own part of the algorithm.  The code for producing this
output is included in the same source file as each of the
{}\texttt{do\_step(...)}  functions for each {}\textit{\texttt{AlgorithmStep}}
subclass.  Therefore, this documentation is decoupled from other steps as much
as the implementation code is, and maintaining the documentation is more
urgent since it is in the same source file.  An example of this printout for
an rSQP algorithm generated by the ``MamaJama'' configuration\footnote{The
name MamaJama was used for the very first algorithm configuration class and
was meant to be a ``do all'' configuration class for active-set rSQP
algorithms.} is shown in {}\cite{ref:moochorefguide}.  Each Step object is
given a name that other steps refer to it by (to initiate minor loops for
instance).  Also, the name of the concrete subclass which implements each step
is included as a guide to help track down the implementations.

For a more detailed look at the output files {}\texttt{Moocho\-Algo.out} and
{}\texttt{Moocho\-Journal.out} see the MOOCHO Reference Manual
{}\cite{ref:moochorefguide}.

%
\subsection{Algorithm Summary and Timing}
%

In addition to the more detailed information that can be printed to the file
{}\texttt{Moocho\-Journal.out}, summary information about each MOOCHO
iteration is printed to the file {}\texttt{Moocho\-Summary.out}.  Also, if the
option {}\texttt{Moocho\-Solver\{\-algo\_timing\-=true\-\}} is set, then this
file will also get a summary table of the run-times and statistics for each
step.  These timings are printed out in a tabular format giving the time, in
seconds, each step consumed for each iteration as well as the sum of the times
of all the steps.  See the MOOCHO Reference Manual {}\cite{ref:moochorefguide}
for an example of a {}\texttt{Moocho\-Summary.out} file.

This timing information can be used to determine where the bottlenecks are in
the algorithm for a particular NLP.  Of course, for very small NLPs the
runtime is dominated by overhead and not numerical computations, so timing of
small problems is not terribly interesting or useful.

%
\subsection{Algorithm and NLP Testing and Validation}
\label{moocho:sec:testing_and_validation}
%

Many computations are performed in order to solve a nonlinear program (NLP)
using a numerical optimization method.  If there is a significant error
(programming bug or excessive round-off) in any step of the computation, the
numerical algorithm will not be able to solve the NLP, or at least not to a
satisfactory tolerance.  When a user goes to solve a NLP that the user has
written and the optimization algorithm fails or the solution found does not
seem reasonable, the user is left to wonder what went wrong.  Could the NLP be
coded incorrectly?  Is there a bug in the optimization software that has gone
up till now undetected?  For any non-trivial NLP or optimization algorithm it
is very difficult to diagnose such a problem, especially if the user is not an
expert in optimization.  Even if the user is an expert, the typical
investigative process is still very tedious and time consuming.

Fortunately, it is possible to partially validate the consistency of the NLP
implementation (i.e.~gradients are consistent with function evaluations) as
well as many of the major steps of the optimization algorithm.  Such tests can
be implemented in a way that the added cost (runtime and storage) is of only
the same order as the computations themselves and therefore are not
prohibitively expensive.  There are several possible sources for such errors.
These sources of errors, from the most likely to the least likely are:

\begin{enumerate}
\item Errors in the NLP implementation (e.g.\ bad derivatives)
\item Errors in the user specialized parts of the optimization algorithm
(e.g.\ a bad specialized {}\texttt{\textit{BasisSystem}} object)
\item Errors in the core optimization code (e.g.\ errors in mathematics,
programming logic, or memory usage)
\item Or, errors in the compiler or runtime environment (e.g.\ excessive
roundoff due to overly aggressive compiler optimizations).
\end{enumerate}

There are many ways to make a mistake in coding the NLP interface.  For
instance, assuming the user's NLP model is valid (i.e.~continuous and
differentiable), the user may have made a mistake in writing the code that
computes $f(x)$, $c(x)$, $\nabla f(x)$ and/or $\nabla c(x)$.  Suppose the
gradient of the constraints matrix $\nabla c$ is not consistent with $c(x)$
but only in some regions.  The matrix $\nabla c$ may be used by a generic
{}\texttt{\textit{Basis\-System}} object to find and factor the basis matrix
$C$ and therefore, the entire algorithm would be affected.  To validate
$\nabla c$, the entire matrix could be approximately computed by finite
differences of course and then compared to the $\nabla c$ computed by the NLP
interface, but this would be far too expensive in runtime ($O(n m)$) and
storage ($O(n m)$) costs for larger NLPs.  Computing each individual component
of the derivatives $\nabla f$ and $\nabla c$ by finite differences is an
option but it must be explicitly turned on (see the option
{}\texttt{NLP\-First\-Deriv\-Tester\{\-fd\-\_testing\-\_method\-=FD\-\_COMPUTE\-\_ALL\-\}}).
As a compromise, by default, directional finite differencing can be used to
show that $\nabla c$ is not consistent with $c(x)$, but can not strictly prove
that $\nabla c$ is completely correct.  This works as follows.  The
optimization algorithm asks the NLP interface to compute $\nabla c_k$ at a
point $x_k$.  Then, at the same point $x_k$, for a random vector $v$, the
matrix-vector product $\nabla c(x_k) v$ is approximated, using central finite
differences for instance, as $\nabla c(x_k) v \approx t_1 = ( c(x_k + h v) -
c(x_k - h v) ) / 2 h$ where $h \approx 10^{-5}$ (where $h$ can be set by the
user through the options in the options group
{}\texttt{Calc\-Finite\-Diff\-Prod}).  Then the matrix-vector product $t_2 =
\nabla c_k v$ would be computed using the $\nabla c_k$ matrix object computed
by the NLP interface and the resultant vectors $t_1$ and $t_2$ are then
compared.  Even if the user did an exemplary job of implementing the NLP
interface, the computed $t_1$ and $t_2$ vectors will not be exactly equal
(i.e.~$t_1 \neq t_2$) due to unavoidable round-off errors (and truncation
errors in the finite-difference computation).  Therefore, we need some type of
measure of how well $t_1$ and $t_2$ compare.  For every such test in MOOCHO
there are defined an error tolerance {}\texttt{error\_tol} and warning
tolerance {}\texttt{warning\_tol} that are adjustable by the user.  Any
relative error greater than {}\texttt{error\_tol} will cause the optimization
algorithm to be terminated with an error message printed.  Any relative error
greater than {}\texttt{warning\_tol} will be printed to the journal file to
warn the user of some possible problems.  For example, relative errors greater
than {}\texttt{warning\_tol} = $10^{-12}$ but smaller than
{}\texttt{error\_tol} = $10^{-8}$ may concern us, but the algorithm still may
be able to solve the NLP.  The finite-difference testing of the NLP interface
can be controlled by setting options in the
{}\texttt{NLP\-First\-Deriv\-Tester} and {}\texttt{Calc\-Finite\-Diff\-Prod}
options groups and up to fourth-order central differences are supported (and
are the default), which yield very accurate derivatives in many cases.
Testing the NLP's interface at just one point, such as the initial guess
$x^0$, is not sufficient to validate the NLP interface.  For example, suppose
we have a constraint $c_{10}(x) = x_2^3$ with $\partial c_{10} / \partial x_2
= 3 x_2 ^ 2$.  If the derivative was coded as $\partial c_{10} / \partial x_2
= 3 x_2$ by accident, this would appear exactly correct at the points $x_2 =
0$ and $x_2 = 1$ but would not be correct for any other values of $x_2$.
Therefore, it is important to test the NLP interface at {}\underline{every}
SQP iteration if one really wants to validate the NLP interface.  Of course,
just because the NLP interface is consistent, does not mean it implements the
model the user had in mind, but this is a different matter.  If the NLP is
unbounded or infeasible, the SQP algorithm will determine this (but the error
message produced by the algorithm may not be able to state exactly the cause
of the problem).

Every major computation in a SQP algorithm can be validated, at least
partially, with little extra cost.  For example, an interface that is used to
solve for a linear system $x = A^{-1} b$ such as the
{}\texttt{\textit{Matrix\-Op\-Nonsing}} can be checked by computing $q = A x$
and then comparing $q$ to $b$.  Computations can also be validated for the
null-space decomposition (see {}\texttt{Decomposition\-System\-Tester}) and QP
solver (see {}\texttt{QPSolver\-Relaxed\-Tester}) objects.  Since
sophisticated users can come in and replace any of these objects, it is a good
idea to be able to test everything that can realistically be tested whenever
the correctness of the algorithm is in question or new objects are being
integrated and tested.  Much of this testing code is already in place in
MOOCHO, but more is needed for more complete validation.  The option
{}\texttt{NLP\-Solver\-Client\-Interface\{check\-\_results=true\}} will turn
on all such runtime checks and, with default settings, should be only increase
the cost of the algorithm by a constant factor, independent of the size of the
problem.

Such careful testing and validation code can save lots of debugging time and
also help avoid reporting incorrect results which can be embarrassing in an
academic research setting or costly in a real-world setting.  Testing and
validation is no small matter and should be taken seriously, especially in a
dynamic environment with lots of variability like MOOCHO.

%
\subsection{Algorithm Interruption}
\label{moocho:sec:interruption}
%

All MOOCHO algorithms can be interrupted at any time while the algorithm is
running and result in a graceful termination, even for parallel runs with MPI.
When running in interactive mode (i.e. the user has access to standard in and
standard out in the console) then typing {}\texttt{Ctrl-C} will cause the
algorithm to pause and allow the user to enter termination criteria on the
standard input stream.  Or, a MOOCHO algorithm can be interrupted without
access to standard in or standard out (i.e.\ when running in batch mode) by
setting up an interrupt file.  This feature allows the client to terminate a
MOOCHO algorithm at any time and still result in a graceful exit where the
current status of the solution is compiled and returned to the user (through
the NLP interface).  See the MOOCHO Reference Manual
{}\cite{ref:moochorefguide} for more details.

%
\section{Summary}
%

MOOCHO currently implements several variates of reduced-space quasi-Newton
successive quadratic programming methods.  MOOCHO is written in C++ and can be
used to address very large-scale optimization problems.  MOOCHO is distributed
as part of the Trilinos {}\cite{ref:trilinos} collection.  Both general serial
problems and massively parallel simulation-constrained problems can be
addressed with MOOCHO.  Parallel simulation-constrained problems can be
represented through Thyra and can utilize a great deal of the parallel (and
serial) linear solver capability present through Trilinos.


% ---------------------------------------------------------------------- %
% References
%
\clearpage
% If hyperref is included, then \phantomsection is already defined.
% If not, we need to define it.
\providecommand*{\phantomsection}{}
\phantomsection
\bibliographystyle{plain}
\bibliography{references}
\addcontentsline{toc}{chapter}{References}


% ---------------------------------------------------------------------- %
%
\appendix
\input{apdx_MoochoEqnGuide}

% \printindex

\begin{SANDdistribution}[NM]
\end{SANDdistribution}

\end{document}
